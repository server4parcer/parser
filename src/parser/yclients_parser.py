"""
YCLIENTS Parser - ÐžÑÐ½Ð¾Ð²Ð½Ð¾Ð¹ Ð¼Ð¾Ð´ÑƒÐ»ÑŒ Ð¿Ð°Ñ€ÑÐ¸Ð½Ð³Ð° Ð´Ð°Ð½Ð½Ñ‹Ñ… Ñ Ð¿Ð»Ð°Ñ‚Ñ„Ð¾Ñ€Ð¼Ñ‹ YCLIENTS.
"""
import asyncio
import logging
import time
from datetime import datetime
from typing import Dict, List, Optional, Any, Tuple

from playwright.async_api import async_playwright, Browser, BrowserContext, Page, TimeoutError
import re

from src.browser.browser_manager import BrowserManager
from src.browser.proxy_manager import ProxyManager
from src.database.db_manager import DatabaseManager
from src.parser.production_data_extractor import ProductionDataExtractor
from src.parser.yclients_real_selectors import YCLIENTS_REAL_SELECTORS
from config.settings import PARSE_INTERVAL, MAX_RETRIES, TIMEOUT, USER_AGENTS, PAGE_LOAD_TIMEOUT


logger = logging.getLogger(__name__)


class YClientsParser:
    """
    ÐžÑÐ½Ð¾Ð²Ð½Ð¾Ð¹ ÐºÐ»Ð°ÑÑ Ð´Ð»Ñ Ð¿Ð°Ñ€ÑÐ¸Ð½Ð³Ð° Ð´Ð°Ð½Ð½Ñ‹Ñ… Ñ YCLIENTS.
    Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ Playwright Ð´Ð»Ñ Ñ€Ð°Ð±Ð¾Ñ‚Ñ‹ Ñ Ð²ÐµÐ±-ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ð°Ð¼Ð¸ Ð¸ ÑÐ¼ÑƒÐ»ÑÑ†Ð¸Ð¸ Ð¿Ð¾Ð²ÐµÐ´ÐµÐ½Ð¸Ñ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ.
    """

    def __init__(self, urls: List[str], db_manager: DatabaseManager):
        """
        Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð¿Ð°Ñ€ÑÐµÑ€Ð°.
        
        Args:
            urls: Ð¡Ð¿Ð¸ÑÐ¾Ðº URL-Ð°Ð´Ñ€ÐµÑÐ¾Ð² Ð´Ð»Ñ Ð¿Ð°Ñ€ÑÐ¸Ð½Ð³Ð°
            db_manager: Ð­ÐºÐ·ÐµÐ¼Ð¿Ð»ÑÑ€ Ð¼ÐµÐ½ÐµÐ´Ð¶ÐµÑ€Ð° Ð±Ð°Ð·Ñ‹ Ð´Ð°Ð½Ð½Ñ‹Ñ…
        """
        self.urls = urls
        self.db_manager = db_manager
        self.browser_manager = BrowserManager()
        self.proxy_manager = ProxyManager()
        # Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ production-ready ÑÐºÑÑ‚Ñ€Ð°ÐºÑ‚Ð¾Ñ€ Ð´Ð°Ð½Ð½Ñ‹Ñ…
        self.data_extractor = ProductionDataExtractor()
        self.browser: Optional[Browser] = None
        self.context: Optional[BrowserContext] = None
        self.page: Optional[Page] = None
        self.current_proxy = None
        self.retry_count = 0
        self.last_parsed_urls = {}  # Ð”Ð»Ñ Ð¾Ñ‚ÑÐ»ÐµÐ¶Ð¸Ð²Ð°Ð½Ð¸Ñ ÑƒÑÐ¿ÐµÑˆÐ½Ð¾ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ð½Ð½Ñ‹Ñ… URL
        self.captured_api_data = []  # Shared list for API responses captured during page navigation
        self.scraped_providers = []  # HTML-scraped provider/court names for 100% business value

    async def initialize(self) -> None:
        """Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð±Ñ€Ð°ÑƒÐ·ÐµÑ€Ð° Ð¸ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ð°."""
        try:
            logger.info("Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð±Ñ€Ð°ÑƒÐ·ÐµÑ€Ð°")
            
            # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ Ð¿Ñ€Ð¾ÐºÑÐ¸ Ð´Ð»Ñ Ñ‚ÐµÐºÑƒÑ‰ÐµÐ¹ ÑÐµÑÑÐ¸Ð¸
            self.current_proxy = self.proxy_manager.get_next_proxy()
            
            # Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÐµÐ¼ Ð±Ñ€Ð°ÑƒÐ·ÐµÑ€ Ñ Ð½Ð°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ°Ð¼Ð¸ ÑÑ‚ÐµÐ»Ñ-Ñ€ÐµÐ¶Ð¸Ð¼Ð°
            self.browser, self.context = await self.browser_manager.initialize_browser(
                proxy=self.current_proxy
            )
            
            logger.info("Ð‘Ñ€Ð°ÑƒÐ·ÐµÑ€ ÑƒÑÐ¿ÐµÑˆÐ½Ð¾ Ð¸Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½")
        except Exception as e:
            logger.error(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð¸Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ð¸ Ð±Ñ€Ð°ÑƒÐ·ÐµÑ€Ð°: {str(e)}")
            raise

    async def close(self) -> None:
        """Ð—Ð°ÐºÑ€Ñ‹Ñ‚Ð¸Ðµ Ð±Ñ€Ð°ÑƒÐ·ÐµÑ€Ð° Ð¸ Ð¾ÑÐ²Ð¾Ð±Ð¾Ð¶Ð´ÐµÐ½Ð¸Ðµ Ñ€ÐµÑÑƒÑ€ÑÐ¾Ð²."""
        try:
            if self.context:
                await self.context.close()
            if self.browser:
                await self.browser.close()
            logger.info("Ð‘Ñ€Ð°ÑƒÐ·ÐµÑ€ Ð¸ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚ Ð·Ð°ÐºÑ€Ñ‹Ñ‚Ñ‹")
        except Exception as e:
            logger.error(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ Ð·Ð°ÐºÑ€Ñ‹Ñ‚Ð¸Ð¸ Ð±Ñ€Ð°ÑƒÐ·ÐµÑ€Ð°: {str(e)}")

    async def navigate_to_url(self, url: str) -> bool:
        """
        ÐŸÐµÑ€ÐµÑ…Ð¾Ð´ Ð¿Ð¾ URL Ñ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¾Ð¹ Ð¾ÑˆÐ¸Ð±Ð¾Ðº Ð¸ Ð¿Ð¾Ð²Ñ‚Ð¾Ñ€Ð½Ñ‹Ð¼Ð¸ Ð¿Ð¾Ð¿Ñ‹Ñ‚ÐºÐ°Ð¼Ð¸.
        
        Args:
            url: URL ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ñ‹ Ð´Ð»Ñ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸
            
        Returns:
            bool: True ÐµÑÐ»Ð¸ Ð¿ÐµÑ€ÐµÑ…Ð¾Ð´ ÑƒÑÐ¿ÐµÑˆÐµÐ½, False Ð² Ð¿Ñ€Ð¾Ñ‚Ð¸Ð²Ð½Ð¾Ð¼ ÑÐ»ÑƒÑ‡Ð°Ðµ
        """
        try:
            # Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ Ð½Ð¾Ð²ÑƒÑŽ ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ñƒ Ð² Ñ‚ÐµÐºÑƒÑ‰ÐµÐ¼ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ðµ
            self.page = await self.context.new_page()
            
            # Ð£ÑÑ‚Ð°Ð½Ð°Ð²Ð»Ð¸Ð²Ð°ÐµÐ¼ ÑÐ»ÑƒÑ‡Ð°Ð¹Ð½Ñ‹Ð¹ ÑŽÐ·ÐµÑ€-Ð°Ð³ÐµÐ½Ñ‚ Ð¸Ð· ÑÐ¿Ð¸ÑÐºÐ° Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ñ‹Ñ…
            user_agent = self.browser_manager.get_random_user_agent()
            await self.page.set_extra_http_headers({"User-Agent": user_agent})

            # ========== API REQUEST LOGGING AND CAPTURE FOR SPA ==========
            # Clear previously captured data for new page
            self.captured_api_data = []

            async def capture_and_log_api(response):
                """Capture API responses AND log them for debugging"""
                url = response.url

                # Log ALL API calls for debugging
                if any(keyword in url for keyword in ['api', 'booking', 'slot', 'availability', 'time', 'service', 'calendar', 'ajax', 'data']):
                    logger.info(f"ðŸŒ [API-CALL] {response.status} {response.request.method} {url}")

                    # Try to capture and log response data
                    try:
                        if response.status == 200:
                            content_type = response.headers.get('content-type', '')

                            if 'application/json' in content_type:
                                data = await response.json()
                                logger.info(f"ðŸŒ [API-DATA] JSON response keys: {list(data.keys()) if isinstance(data, dict) else 'array'}")

                                # Log sample data
                                if isinstance(data, list) and len(data) > 0:
                                    logger.info(f"ðŸŒ [API-SAMPLE] First item: {str(data[0])[:200]}")
                                elif isinstance(data, dict):
                                    logger.info(f"ðŸŒ [API-SAMPLE] Data: {str(data)[:200]}")

                                # Ð—Ð°Ñ…Ð²Ð°Ñ‚Ñ‹Ð²Ð°ÐµÐ¼ Ð’Ð¡Ð• API Ð´Ð»Ñ ÐºÐ¾Ñ€Ñ€ÐµÐ»ÑÑ†Ð¸Ð¸ Ð´Ð°Ð½Ð½Ñ‹Ñ…
                                # search-timeslots: Ð²Ñ€ÐµÐ¼Ñ Ð±Ñ€Ð¾Ð½Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ (datetime, time)
                                # search-services: Ñ†ÐµÐ½Ñ‹ Ð¸ Ð½Ð°Ð·Ð²Ð°Ð½Ð¸Ñ ÑƒÑÐ»ÑƒÐ³ (price_min, price_max, service_name)
                                # search-staff: Ð¸Ð¼ÐµÐ½Ð° Ð¼Ð°ÑÑ‚ÐµÑ€Ð¾Ð²/ÐºÐ¾Ñ€Ñ‚Ð¾Ð² (staff_name)
                                # search-dates: Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ñ‹Ðµ Ð´Ð°Ñ‚Ñ‹
                                if any(keyword in url for keyword in [
                                    'search-timeslots',   # Ð’Ñ€ÐµÐ¼Ñ Ð±Ñ€Ð¾Ð½Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ
                                    'search-services',    # Ð¦ÐµÐ½Ñ‹ Ð¸ Ð½Ð°Ð·Ð²Ð°Ð½Ð¸Ñ ÑƒÑÐ»ÑƒÐ³
                                    'search-staff',       # ÐŸÑ€Ð¾Ð²Ð°Ð¹Ð´ÐµÑ€Ñ‹/ÐºÐ¾Ñ€Ñ‚Ñ‹
                                    'search-dates',       # Ð”Ð¾ÑÑ‚ÑƒÐ¿Ð½Ñ‹Ðµ Ð´Ð°Ñ‚Ñ‹
                                ]):
                                    # Identify API type
                                    api_type = 'UNKNOWN'
                                    if 'search-timeslots' in url:
                                        api_type = 'TIMESLOTS'
                                    elif 'search-services' in url:
                                        api_type = 'SERVICES'
                                    elif 'search-staff' in url:
                                        api_type = 'STAFF'
                                    elif 'search-dates' in url:
                                        api_type = 'DATES'

                                    logger.info(f"ðŸŒ [API-CAPTURE] âœ… Captured {api_type} from: {url}")

                                    # Log data structure details
                                    if isinstance(data, dict) and 'data' in data:
                                        items = data['data'] if isinstance(data['data'], list) else [data['data']]
                                        logger.info(f"ðŸŒ [API-CAPTURE] {api_type} has {len(items)} items")
                                        if items and len(items) > 0:
                                            first_item = items[0]
                                            if isinstance(first_item, dict) and 'attributes' in first_item:
                                                attrs = first_item['attributes']
                                                logger.info(f"ðŸŒ [API-CAPTURE] {api_type} first item keys: {list(attrs.keys())}")

                                    self.captured_api_data.append({
                                        'api_url': url,
                                        'data': data,
                                        'timestamp': datetime.now().isoformat()
                                    })
                    except Exception as e:
                        logger.debug(f"Could not parse API response: {e}")

            # Attach listener to page
            self.page.on('response', capture_and_log_api)
            logger.info("ðŸŒ [INIT] Network request listener attached (with capture)")
            # ========== END API REQUEST LOGGING AND CAPTURE ==========

            # Ð­Ð¼ÑƒÐ»ÑÑ†Ð¸Ñ Ð¿Ð¾Ð²ÐµÐ´ÐµÐ½Ð¸Ñ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ: ÑÐ»ÑƒÑ‡Ð°Ð¹Ð½Ñ‹Ðµ Ð·Ð°Ð´ÐµÑ€Ð¶ÐºÐ¸ Ð¿ÐµÑ€ÐµÐ´ Ð½Ð°Ð²Ð¸Ð³Ð°Ñ†Ð¸ÐµÐ¹
            await asyncio.sleep(self.browser_manager.get_random_delay(1, 3))
            
            logger.info(f"ÐŸÐµÑ€ÐµÑ…Ð¾Ð´ Ð¿Ð¾ URL: {url}")
            
            # Ð£ÑÑ‚Ð°Ð½Ð°Ð²Ð»Ð¸Ð²Ð°ÐµÐ¼ Ñ‚Ð°Ð¹Ð¼Ð°ÑƒÑ‚ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸ ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ñ‹
            response = await self.page.goto(
                url, 
                timeout=PAGE_LOAD_TIMEOUT,
                wait_until="networkidle"
            )
            
            if not response or response.status >= 400:
                logger.error(f"ÐÐµÑƒÐ´Ð°Ñ‡Ð½Ñ‹Ð¹ Ð·Ð°Ð¿Ñ€Ð¾Ñ Ðº {url}, ÑÑ‚Ð°Ñ‚ÑƒÑ: {response.status if response else 'unknown'}")
                return False
            
            # Ð–Ð´ÐµÐ¼ Ð¿Ð¾Ð»Ð½Ð¾Ð¹ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸ ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ñ‹ Ð¸ Ð´Ð¸Ð½Ð°Ð¼Ð¸Ñ‡ÐµÑÐºÐ¾Ð³Ð¾ ÐºÐ¾Ð½Ñ‚ÐµÐ½Ñ‚Ð°
            await self.page.wait_for_load_state("networkidle")

            # Ð­Ð¼ÑƒÐ»ÑÑ†Ð¸Ñ ÑÐ»ÑƒÑ‡Ð°Ð¹Ð½Ð¾Ð³Ð¾ ÑÐºÑ€Ð¾Ð»Ð»Ð¸Ð½Ð³Ð° ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ñ‹
            await self.browser_manager.emulate_human_scrolling(self.page)

            # ========== HTML PROVIDER SCRAPING FOR 100% BUSINESS VALUE ==========
            # Scrape provider/court names from HTML (APIs don't have them!)
            await self.scrape_provider_names_from_html()
            # ========== END HTML PROVIDER SCRAPING ==========

            return True
            
        except TimeoutError:
            logger.error(f"Ð¢Ð°Ð¹Ð¼Ð°ÑƒÑ‚ Ð¿Ñ€Ð¸ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐµ ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ñ‹: {url}")
            return False
        except Exception as e:
            logger.error(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ Ð½Ð°Ð²Ð¸Ð³Ð°Ñ†Ð¸Ð¸ Ð½Ð° {url}: {str(e)}")
            return False

    async def scrape_provider_names_from_html(self) -> None:
        """
        Scrape provider/court names from HTML page.
        CRITICAL FOR 100% BUSINESS VALUE - APIs don't have service_name/provider fields!

        Strategy:
        - Find all service/court name elements in DOM
        - Extract text + associated data-id/service-id attributes
        - Store for later correlation with API data
        """
        try:
            logger.info("ðŸ·ï¸  [HTML-SCRAPE] Starting provider name extraction from HTML")

            # Clear previous scraped data
            self.scraped_providers = []

            # Wait a bit for dynamic content to fully render
            await asyncio.sleep(1)

            # Execute JavaScript to find all provider/court/service name elements
            providers = await self.page.evaluate('''() => {
                const results = [];

                // Try multiple selector strategies
                const selectors = [
                    // YClients common patterns
                    '.service-name',
                    '.service-title',
                    '.service-card .title',
                    '.service-item .name',
                    '.staff-name',
                    '.staff-title',
                    '.court-name',
                    '.booking-service-name',
                    '[data-service-name]',
                    '[data-court-name]',
                    '[data-service-title]',
                    // Generic patterns
                    '.service h3',
                    '.service h4',
                    '.card-title',
                    '.item-title',
                    // Try data attributes
                    '[data-service-id]',
                    '[data-staff-id]',
                    '[data-id][class*="service"]',
                    '[data-id][class*="court"]'
                ];

                for (const selector of selectors) {
                    try {
                        const elements = document.querySelectorAll(selector);
                        for (const el of elements) {
                            const text = el.textContent?.trim();
                            if (text && text.length > 0 && text.length < 200) {
                                const id = el.dataset.serviceId || el.dataset.staffId ||
                                           el.dataset.id || el.dataset.courtId ||
                                           el.getAttribute('data-service-id') ||
                                           el.getAttribute('data-id');

                                // Only add if we haven't seen this text yet
                                if (!results.some(r => r.name === text)) {
                                    results.push({
                                        name: text,
                                        id: id || null,
                                        selector: selector,
                                        className: el.className
                                    });
                                }
                            }
                        }
                    } catch (e) {
                        // Selector failed, continue
                    }
                }

                return results;
            }''')

            self.scraped_providers = providers

            if providers:
                logger.info(f"ðŸ·ï¸  [HTML-SCRAPE] Found {len(providers)} provider/court names:")
                for provider in providers[:5]:  # Log first 5
                    logger.info(f"   - {provider.get('name')} (id: {provider.get('id')}, selector: {provider.get('selector')})")
                if len(providers) > 5:
                    logger.info(f"   ... and {len(providers) - 5} more")
            else:
                logger.warning("ðŸ·ï¸  [HTML-SCRAPE] No provider names found in HTML (may need manual selector inspection)")

        except Exception as e:
            logger.error(f"ðŸ·ï¸  [HTML-SCRAPE] Error scraping providers: {e}")
            self.scraped_providers = []

    async def handle_service_selection_page(self, url: str) -> List[str]:
        """
        ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ñ‹ Ð²Ñ‹Ð±Ð¾Ñ€Ð° ÑƒÑÐ»ÑƒÐ³ Ð´Ð»Ñ Ð¿Ð¾Ð»ÑƒÑ‡ÐµÐ½Ð¸Ñ Ð¿Ñ€ÑÐ¼Ñ‹Ñ… ÑÑÑ‹Ð»Ð¾Ðº Ð½Ð° Ð±Ñ€Ð¾Ð½Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ.
        Ð ÐµÑˆÐ°ÐµÑ‚ Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼Ñƒ Ñ€ÐµÐ´Ð¸Ñ€ÐµÐºÑ‚Ð° Ñ URL Ñ‚Ð¸Ð¿Ð° record-type?o=
        
        Args:
            url: URL ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ñ‹ Ð²Ñ‹Ð±Ð¾Ñ€Ð° ÑƒÑÐ»ÑƒÐ³
            
        Returns:
            List[str]: Ð¡Ð¿Ð¸ÑÐ¾Ðº Ð¿Ñ€ÑÐ¼Ñ‹Ñ… URL Ð´Ð»Ñ Ð±Ñ€Ð¾Ð½Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ ÐºÐ¾Ð½ÐºÑ€ÐµÑ‚Ð½Ñ‹Ñ… ÑƒÑÐ»ÑƒÐ³
        """
        logger.info(f"ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ñ‹ Ð²Ñ‹Ð±Ð¾Ñ€Ð° ÑƒÑÐ»ÑƒÐ³: {url}")
        direct_urls = []
        
        try:
            # ÐŸÐµÑ€ÐµÑ…Ð¾Ð´Ð¸Ð¼ Ð½Ð° ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ñƒ Ð²Ñ‹Ð±Ð¾Ñ€Ð° ÑƒÑÐ»ÑƒÐ³
            navigation_success = await self.navigate_to_url(url)
            if not navigation_success:
                logger.error(f"ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ð·Ð°Ð³Ñ€ÑƒÐ·Ð¸Ñ‚ÑŒ ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ñƒ Ð²Ñ‹Ð±Ð¾Ñ€Ð° ÑƒÑÐ»ÑƒÐ³: {url}")
                return []
            
            # Ð–Ð´ÐµÐ¼ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸ ÑÐ¿Ð¸ÑÐºÐ° ÑƒÑÐ»ÑƒÐ³
            try:
                await self.page.wait_for_selector('.service-item, .service-option, .record__service', timeout=10000)
            except Exception:
                logger.warning("ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ð´Ð¾Ð¶Ð´Ð°Ñ‚ÑŒÑÑ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸ ÑÐ¿Ð¸ÑÐºÐ° ÑƒÑÐ»ÑƒÐ³")
                return []
            
            # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ Ð²ÑÐµ Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ñ‹Ðµ ÑƒÑÐ»ÑƒÐ³Ð¸
            service_selectors = [
                '.service-item', '.service-option', '.record__service',
                '.ycwidget-service', '.booking-service-item'
            ]
            
            service_elements = []
            for selector in service_selectors:
                elements = await self.page.query_selector_all(selector)
                if elements:
                    service_elements = elements
                    logger.info(f"ÐÐ°Ð¹Ð´ÐµÐ½Ð¾ {len(elements)} ÑƒÑÐ»ÑƒÐ³ Ñ ÑÐµÐ»ÐµÐºÑ‚Ð¾Ñ€Ð¾Ð¼: {selector}")
                    break
            
            if not service_elements:
                logger.warning("ÐÐµ Ð½Ð°Ð¹Ð´ÐµÐ½Ñ‹ ÑÐ»ÐµÐ¼ÐµÐ½Ñ‚Ñ‹ ÑƒÑÐ»ÑƒÐ³ Ð½Ð° ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ðµ")
                return []
            
            # Ð”Ð»Ñ ÐºÐ°Ð¶Ð´Ð¾Ð¹ ÑƒÑÐ»ÑƒÐ³Ð¸ Ð¿Ð¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ Ð¿Ñ€ÑÐ¼ÑƒÑŽ ÑÑÑ‹Ð»ÐºÑƒ
            for i, service_element in enumerate(service_elements[:5]):  # ÐžÐ³Ñ€Ð°Ð½Ð¸Ñ‡Ð¸Ð²Ð°ÐµÐ¼ ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ Ð´Ð»Ñ Ð±ÐµÐ·Ð¾Ð¿Ð°ÑÐ½Ð¾ÑÑ‚Ð¸
                try:
                    # ÐŸÑ€Ð¾Ð±ÑƒÐµÐ¼ Ð½Ð°Ð¹Ñ‚Ð¸ ÑÑÑ‹Ð»ÐºÑƒ Ð²Ð½ÑƒÑ‚Ñ€Ð¸ ÑÐ»ÐµÐ¼ÐµÐ½Ñ‚Ð°
                    link_element = await service_element.query_selector('a')
                    if link_element:
                        href = await link_element.get_attribute('href')
                        if href:
                            if href.startswith('/'):
                                # ÐŸÑ€ÐµÐ¾Ð±Ñ€Ð°Ð·ÑƒÐµÐ¼ Ð¾Ñ‚Ð½Ð¾ÑÐ¸Ñ‚ÐµÐ»ÑŒÐ½ÑƒÑŽ ÑÑÑ‹Ð»ÐºÑƒ Ð² Ð°Ð±ÑÐ¾Ð»ÑŽÑ‚Ð½ÑƒÑŽ
                                base_url = '/'.join(url.split('/')[:3])
                                direct_url = base_url + href
                            else:
                                direct_url = href
                            
                            if 'record' in direct_url and direct_url not in direct_urls:
                                direct_urls.append(direct_url)
                                logger.info(f"ÐÐ°Ð¹Ð´ÐµÐ½Ð° Ð¿Ñ€ÑÐ¼Ð°Ñ ÑÑÑ‹Ð»ÐºÐ°: {direct_url}")
                                continue
                    
                    # Ð•ÑÐ»Ð¸ ÑÑÑ‹Ð»ÐºÐ¸ Ð½ÐµÑ‚, Ð¿Ñ€Ð¾Ð±ÑƒÐµÐ¼ ÐºÐ»Ð¸ÐºÐ½ÑƒÑ‚ÑŒ Ð½Ð° ÑÐ»ÐµÐ¼ÐµÐ½Ñ‚
                    logger.info(f"ÐšÐ»Ð¸ÐºÐ°ÐµÐ¼ Ð½Ð° ÑƒÑÐ»ÑƒÐ³Ñƒ {i+1}")
                    await service_element.click()
                    await asyncio.sleep(2)  # Ð–Ð´ÐµÐ¼ Ð½Ð°Ð²Ð¸Ð³Ð°Ñ†Ð¸Ð¸
                    
                    # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ URL Ð¿Ð¾ÑÐ»Ðµ ÐºÐ»Ð¸ÐºÐ°
                    current_url = self.page.url
                    if 'record' in current_url and current_url != url and current_url not in direct_urls:
                        direct_urls.append(current_url)
                        logger.info(f"ÐŸÐ¾Ð»ÑƒÑ‡ÐµÐ½Ð° ÑÑÑ‹Ð»ÐºÐ° Ð¿Ð¾ÑÐ»Ðµ ÐºÐ»Ð¸ÐºÐ°: {current_url}")
                    
                    # Ð’Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÐ¼ÑÑ Ð½Ð° ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ñƒ Ð²Ñ‹Ð±Ð¾Ñ€Ð° ÑƒÑÐ»ÑƒÐ³
                    await self.page.go_back()
                    await asyncio.sleep(1)
                    
                except Exception as e:
                    logger.warning(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐµ ÑƒÑÐ»ÑƒÐ³Ð¸ {i+1}: {str(e)}")
                    continue
            
            logger.info(f"ÐŸÐ¾Ð»ÑƒÑ‡ÐµÐ½Ð¾ {len(direct_urls)} Ð¿Ñ€ÑÐ¼Ñ‹Ñ… ÑÑÑ‹Ð»Ð¾Ðº Ð´Ð»Ñ Ð±Ñ€Ð¾Ð½Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ")
            return direct_urls
            
        except Exception as e:
            logger.error(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐµ ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ñ‹ Ð²Ñ‹Ð±Ð¾Ñ€Ð° ÑƒÑÐ»ÑƒÐ³: {str(e)}")
            return []

    async def check_for_antibot(self) -> bool:
        """
        ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð½Ð°Ð»Ð¸Ñ‡Ð¸Ñ Ð°Ð½Ñ‚Ð¸Ð±Ð¾Ñ‚-Ð·Ð°Ñ‰Ð¸Ñ‚Ñ‹ Ð¸ ÐµÑ‘ Ð¾Ð±Ñ…Ð¾Ð´, ÐµÑÐ»Ð¸ Ð²Ð¾Ð·Ð¼Ð¾Ð¶Ð½Ð¾.
        
        Returns:
            bool: True ÐµÑÐ»Ð¸ Ð·Ð°Ñ‰Ð¸Ñ‚Ñ‹ Ð½ÐµÑ‚ Ð¸Ð»Ð¸ Ð¾Ð½Ð° ÑƒÑÐ¿ÐµÑˆÐ½Ð¾ Ð¾Ð±Ð¾Ð¹Ð´ÐµÐ½Ð°, False Ð² Ð¿Ñ€Ð¾Ñ‚Ð¸Ð²Ð½Ð¾Ð¼ ÑÐ»ÑƒÑ‡Ð°Ðµ
        """
        try:
            # ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð½Ð° Ð½Ð°Ð»Ð¸Ñ‡Ð¸Ðµ ÐºÐ°Ð¿Ñ‡Ð¸ Ð¸Ð»Ð¸ Ð´Ñ€ÑƒÐ³Ð¸Ñ… Ñ„Ð¾Ñ€Ð¼ Ð·Ð°Ñ‰Ð¸Ñ‚Ñ‹
            captcha_exists = await self.page.query_selector(".captcha, .recaptcha, .hcaptcha")
            if captcha_exists:
                logger.warning("ÐžÐ±Ð½Ð°Ñ€ÑƒÐ¶ÐµÐ½Ð° CAPTCHA, Ð¿Ð¾Ð¿Ñ‹Ñ‚ÐºÐ° Ð¾Ð±Ñ…Ð¾Ð´Ð°...")
                # Ð—Ð´ÐµÑÑŒ Ð¼Ð¾Ð³Ð»Ð° Ð±Ñ‹ Ð±Ñ‹Ñ‚ÑŒ Ñ€ÐµÐ°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð¾Ð±Ñ…Ð¾Ð´Ð° ÐºÐ°Ð¿Ñ‡Ð¸
                # Ð’ Ð´Ð°Ð½Ð½Ð¾Ð¹ Ñ€ÐµÐ°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ð¸ Ð¿Ñ€Ð¾ÑÑ‚Ð¾ Ð·Ð°ÐºÑ€Ñ‹Ð²Ð°ÐµÐ¼ ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ñƒ Ð¸ Ð¼ÐµÐ½ÑÐµÐ¼ Ð¿Ñ€Ð¾ÐºÑÐ¸
                return False
            
            # ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð½Ð° Ð±Ð»Ð¾ÐºÐ¸Ñ€Ð¾Ð²ÐºÑƒ IP
            blocked_ip = await self.page.query_selector(".blocked, .access-denied, .error-403")
            if blocked_ip:
                logger.warning("IP Ð·Ð°Ð±Ð»Ð¾ÐºÐ¸Ñ€Ð¾Ð²Ð°Ð½, ÑÐ¼ÐµÐ½Ð° Ð¿Ñ€Ð¾ÐºÑÐ¸")
                return False
                
            # Ð”Ñ€ÑƒÐ³Ð¸Ðµ Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÐ¸ Ð½Ð° Ð°Ð½Ñ‚Ð¸Ð±Ð¾Ñ‚-Ð·Ð°Ñ‰Ð¸Ñ‚Ñƒ...
            
            return True
        except Exception as e:
            logger.error(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÐµ Ð°Ð½Ñ‚Ð¸Ð±Ð¾Ñ‚-Ð·Ð°Ñ‰Ð¸Ñ‚Ñ‹: {str(e)}")
            return False

    async def extract_via_api_interception(self, page: Page, url: str) -> List[Dict]:
        """
        Extract booking data by capturing API responses instead of DOM scraping.
        This works for SPA (Single Page Applications) like YClients that load data via JavaScript.

        Args:
            page: Playwright page object
            url: URL to navigate and extract from

        Returns:
            List of extracted booking records
        """
        logger.info("ðŸŒ [API-MODE] Starting API-based extraction")

        try:
            # Use data that was ALREADY captured during page load by the main listener
            # Page is already loaded, wait for any pending API calls to complete
            await page.wait_for_timeout(2000)
            await page.wait_for_load_state('networkidle', timeout=10000)

            # Check if we already have captured data from navigation
            initial_count = len(self.captured_api_data)
            logger.info(f"ðŸŒ [API-MODE] Already captured {initial_count} API responses during page load")

            # Try to interact with page to trigger MORE API calls (if needed)
            # Click any visible date elements to load time slots
            try:
                dates = await page.locator('.calendar-day:not(.disabled)').all()
                if dates and len(dates) > 0:
                    logger.info(f"ðŸŒ [API-MODE] Found {len(dates)} dates, clicking first to trigger more APIs")
                    await dates[0].click(force=True)
                    await page.wait_for_timeout(2000)
                    await page.wait_for_load_state('networkidle')
            except:
                pass

            # Try clicking service items if on menu page
            try:
                services = await page.locator('[class*="service"], [class*="item"]').all()
                if services and len(services) > 0:
                    logger.info(f"ðŸŒ [API-MODE] Found {len(services)} services, clicking first to trigger more APIs")
                    await services[0].click(force=True)
                    await page.wait_for_timeout(2000)
                    await page.wait_for_load_state('networkidle')
            except:
                pass

            # Check how many responses we have now (including new ones from clicks)
            total_captured = len(self.captured_api_data)
            logger.info(f"ðŸŒ [API-MODE] Total captured API responses: {total_captured} ({total_captured - initial_count} new from interactions)")

            # Process ALL captured API data
            if self.captured_api_data:
                return self.parse_api_responses(self.captured_api_data)
            else:
                logger.warning("ðŸŒ [API-MODE] No API data captured")
                return []

        except Exception as e:
            logger.error(f"ðŸŒ [API-MODE] Error: {str(e)}")
            return []

    def parse_api_responses(self, captured_data: List[Dict]) -> List[Dict]:
        """
        Parse captured API responses into booking records.
        This method tries different response structures based on common API patterns.

        Args:
            captured_data: List of captured API responses with metadata

        Returns:
            List of parsed booking records
        """
        logger.info(f"ðŸŒ [API-PARSE] Processing {len(captured_data)} API responses")

        results = []

        # PHASE 1: Separate data by API type for correlation
        services_data = []  # From search-services (has prices, service names)
        staff_data = []     # From search-staff (has provider/court names)
        timeslots_data = [] # From search-timeslots (has dates/times)
        dates_data = []     # From search-dates (has available dates)

        logger.info(f"ðŸ”— [CORRELATION] Step 1: Separating {len(captured_data)} APIs by type")

        # Log all captured API URLs for debugging
        for item in captured_data:
            api_url = item['api_url']
            logger.info(f"ðŸ”— [CORRELATION] Captured API: {api_url}")

        for item in captured_data:
            api_url = item['api_url']
            data = item['data']

            try:
                # Extract items from JSON API format
                items = []
                if isinstance(data, dict) and 'data' in data:
                    items = data['data'] if isinstance(data['data'], list) else [data['data']]
                elif isinstance(data, list):
                    items = data

                # Categorize by API type
                if 'search-services' in api_url:
                    for service in items:
                        if isinstance(service, dict) and 'attributes' in service:
                            services_data.append(service['attributes'])
                        elif isinstance(service, dict):
                            services_data.append(service)
                    logger.info(f"ðŸ”— [CORRELATION] Found {len(items)} services from {api_url}")

                elif 'search-staff' in api_url:
                    for staff in items:
                        if isinstance(staff, dict) and 'attributes' in staff:
                            staff_data.append(staff['attributes'])
                        elif isinstance(staff, dict):
                            staff_data.append(staff)
                    logger.info(f"ðŸ”— [CORRELATION] Found {len(items)} staff from {api_url}")

                elif 'search-timeslots' in api_url:
                    for slot in items:
                        if isinstance(slot, dict) and 'attributes' in slot:
                            timeslots_data.append(slot['attributes'])
                        elif isinstance(slot, dict):
                            timeslots_data.append(slot)
                    logger.info(f"ðŸ”— [CORRELATION] Found {len(items)} timeslots from {api_url}")

                elif 'search-dates' in api_url:
                    for date in items:
                        if isinstance(date, dict) and 'attributes' in date:
                            dates_data.append(date['attributes'])
                        elif isinstance(date, dict):
                            dates_data.append(date)
                    logger.info(f"ðŸ”— [CORRELATION] Found {len(items)} dates from {api_url}")

            except Exception as e:
                logger.warning(f"ðŸ”— [CORRELATION] Failed to categorize {api_url}: {e}")

        # PHASE 2: Correlate data from different APIs
        logger.info(f"ðŸ”— [CORRELATION] Step 2: Merging data - Services:{len(services_data)}, Staff:{len(staff_data)}, Slots:{len(timeslots_data)}")

        # Strategy: Apply service/staff data to all timeslots from same page load
        # Assumption: 1 service + N timeslots = service applies to all slots
        base_service = services_data[0] if services_data else {}
        base_staff = staff_data[0] if staff_data else {}

        if base_service:
            logger.info(f"ðŸ”— [CORRELATION] Base service: {base_service.get('service_name', 'N/A')}, price: {base_service.get('price_min', 'N/A')}")
        if base_staff:
            logger.info(f"ðŸ”— [CORRELATION] Base staff: {base_staff.get('staff_name', 'N/A')}")

        # Deduplication: Track seen records by (date, time, provider) composite key
        seen_records = set()

        # Merge timeslots with service/staff data + HTML-scraped providers
        for slot_data in timeslots_data:
            merged = {
                **slot_data,      # datetime, time, is_bookable
                **base_service,   # price_min, price_max, service_name, duration
                **base_staff      # staff_name
            }

            # ========== PHASE 2.5: MERGE HTML-SCRAPED PROVIDERS FOR 100% VALUE ==========
            # APIs don't have service_name, so use HTML-scraped data!
            service_id = merged.get('id') or base_service.get('id')
            provider_name = None

            if service_id and self.scraped_providers:
                # Try to match by ID
                for provider in self.scraped_providers:
                    if provider.get('id') == str(service_id):
                        provider_name = provider.get('name')
                        logger.info(f"ðŸ·ï¸  [CORRELATION] Matched provider by ID: {provider_name}")
                        break

            # Fallback: If no ID match, use first scraped provider (better than nothing)
            if not provider_name and self.scraped_providers:
                provider_name = self.scraped_providers[0].get('name')
                logger.info(f"ðŸ·ï¸  [CORRELATION] Using first scraped provider (no ID match): {provider_name}")

            # Add provider to merged data
            if provider_name:
                merged['provider'] = provider_name
            # ========== END HTML-SCRAPED PROVIDERS MERGE ==========

            logger.info(f"ðŸ”— [CORRELATION] Merged slot: time={merged.get('time')}, price={merged.get('price_min')}, provider={merged.get('provider', 'N/A')}")
            result = self.parse_booking_from_api(merged, 'correlated-api')
            if result:
                # Deduplication check using (date, time, provider) composite key
                dedup_key = (result.get('date'), result.get('time'), result.get('provider'))

                # Only add if unique AND has date+time (provider can be None/fallback)
                if dedup_key not in seen_records and result.get('date') and result.get('time'):
                    results.append(result)
                    seen_records.add(dedup_key)
                    logger.info(f"âœ… [DEDUP] Added unique record: date={dedup_key[0]}, time={dedup_key[1]}, provider={dedup_key[2]}")
                else:
                    if dedup_key in seen_records:
                        logger.warning(f"âš ï¸ [DEDUP] Skipped duplicate: {dedup_key}")
                    else:
                        logger.warning(f"âš ï¸ [DEDUP] Skipped incomplete record (missing key fields): {dedup_key}")

        # If we have results from correlation, return them
        if results:
            logger.info(f"ðŸ”— [CORRELATION] Successfully correlated {len(results)} records")
            return results

        # PHASE 3: Fallback to old logic if correlation produced no results
        logger.info(f"ðŸ”— [CORRELATION] No correlated results, falling back to direct parsing")

        for item in captured_data:
            api_url = item['api_url']
            data = item['data']

            logger.info(f"ðŸŒ [API-PARSE] Processing response from: {api_url}")

            try:
                # Try different response structures
                # Structure 1: YClients JSON API format {data: [{type, id, attributes: {...}}]}
                if isinstance(data, dict) and 'data' in data:
                    items = data['data']
                    if isinstance(items, list):
                        logger.info(f"ðŸ” [API-PARSE] Found {len(items)} items in data array for {api_url}")
                        for idx, booking in enumerate(items):
                            # Check if this is JSON API format with attributes
                            if isinstance(booking, dict) and 'attributes' in booking:
                                # Extract the actual data from attributes
                                booking_data = booking['attributes']
                                # Also include type and id for context
                                booking_data['_type'] = booking.get('type')
                                booking_data['_id'] = booking.get('id')
                                logger.info(f"ðŸ” [API-PARSE] Item {idx+1}: type={booking.get('type')}, attributes keys={list(booking_data.keys())}")
                                result = self.parse_booking_from_api(booking_data, api_url)
                            else:
                                # Standard format
                                logger.info(f"ðŸ” [API-PARSE] Item {idx+1}: standard format, keys={list(booking.keys()) if isinstance(booking, dict) else 'not dict'}")
                                result = self.parse_booking_from_api(booking, api_url)
                            if result:
                                results.append(result)
                                logger.info(f"âœ… [API-PARSE] Successfully added item {idx+1}")
                            else:
                                logger.warning(f"âš ï¸ [API-PARSE] Item {idx+1} returned None (filtered out)")

                # Structure 2: {result: {slots: [...]}}
                elif isinstance(data, dict) and 'result' in data:
                    result_data = data['result']
                    if isinstance(result_data, dict) and 'slots' in result_data:
                        for booking in result_data['slots']:
                            result = self.parse_booking_from_api(booking, api_url)
                            if result:
                                results.append(result)
                    elif isinstance(result_data, list):
                        for booking in result_data:
                            result = self.parse_booking_from_api(booking, api_url)
                            if result:
                                results.append(result)

                # Structure 3: [{time, price, available}] - direct array
                elif isinstance(data, list):
                    for booking in data:
                        result = self.parse_booking_from_api(booking, api_url)
                        if result:
                            results.append(result)

                # Structure 4: Direct object
                elif isinstance(data, dict):
                    result = self.parse_booking_from_api(data, api_url)
                    if result:
                        results.append(result)

            except Exception as e:
                logger.warning(f"ðŸŒ [API-PARSE] Failed to parse response structure: {e}")

        logger.info(f"ðŸŒ [API-PARSE] Extracted {len(results)} booking records from API")
        return results

    def parse_booking_from_api(self, booking_obj: Dict, api_url: str) -> Optional[Dict]:
        """
        Parse individual booking object from API response.
        Tries common field names used in booking APIs.

        Args:
            booking_obj: Dictionary containing booking data
            api_url: Source API URL for reference

        Returns:
            Parsed booking dict or None if insufficient data
        """
        try:
            # YClients Ð¿Ñ€ÐµÐ´Ð¾ÑÑ‚Ð°Ð²Ð»ÑÐµÑ‚ Ð¿Ð¾Ð»Ðµ 'time' Ð½Ð°Ð¿Ñ€ÑÐ¼ÑƒÑŽ - Ð˜Ð¡ÐŸÐžÐ›Ð¬Ð—Ð£Ð•Ðœ Ð•Ð“Ðž!
            # ÐžÑ‚Ð²ÐµÑ‚ API: {'datetime': '2025-10-02T08:00:00+03:00', 'time': '8:00', 'is_bookable': True}

            # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ time Ð½Ð°Ð¿Ñ€ÑÐ¼ÑƒÑŽ Ð¸Ð· YClients (Ð½Ð°Ð¸Ð±Ð¾Ð»ÐµÐµ Ð½Ð°Ð´ÐµÐ¶Ð½Ñ‹Ð¹ ÑÐ¿Ð¾ÑÐ¾Ð±)
            result_time = booking_obj.get('time')
            result_date = None

            # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ Ð´Ð°Ñ‚Ñƒ Ð¸Ð· Ð¿Ð¾Ð»Ñ datetime
            datetime_str = booking_obj.get('datetime', '')
            if datetime_str and 'T' in datetime_str:
                try:
                    result_date = datetime_str.split('T')[0]  # "2025-10-02"
                    # Ð•ÑÐ»Ð¸ time Ð½Ðµ Ð¿Ñ€ÐµÐ´Ð¾ÑÑ‚Ð°Ð²Ð»ÐµÐ½ Ð½Ð°Ð¿Ñ€ÑÐ¼ÑƒÑŽ, Ð¿Ð°Ñ€ÑÐ¸Ð¼ Ð¸Ð· datetime
                    if not result_time:
                        time_part = datetime_str.split('T')[1] if len(datetime_str.split('T')) > 1 else ''
                        result_time = time_part.split('+')[0].split('-')[0][:5]  # "08:00"
                    logger.info(f"[PARSE-DEBUG] datetime={datetime_str} -> date={result_date}, time={result_time}")
                except Exception as e:
                    logger.error(f"[PARSE-DEBUG] Failed to parse datetime '{datetime_str}': {e}")

            # Ð ÐµÐ·ÐµÑ€Ð²Ð½Ñ‹Ðµ Ð²Ð°Ñ€Ð¸Ð°Ð½Ñ‚Ñ‹ Ð´Ð»Ñ Ð¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ð²ÑƒÑŽÑ‰Ð¸Ñ… Ð¿Ð¾Ð»ÐµÐ¹
            if not result_date:
                result_date = booking_obj.get('date') or booking_obj.get('booking_date')
            if not result_time:
                result_time = booking_obj.get('slot_time') or booking_obj.get('start_time')

            logger.info(f"[DIRECT-USE] Final values: date={result_date}, time={result_time}")

            result = {
                'url': api_url,
                'date': result_date,
                'time': result_time,
                'price': (booking_obj.get('price') or
                         booking_obj.get('cost') or
                         booking_obj.get('amount') or
                         booking_obj.get('price_min') or
                         booking_obj.get('price_max')),
                'provider': (booking_obj.get('provider') or
                            booking_obj.get('master') or
                            booking_obj.get('staff') or
                            booking_obj.get('staff_name') or
                            booking_obj.get('service_name')),
                'duration': booking_obj.get('duration', 60),
                'available': booking_obj.get('available') or booking_obj.get('is_bookable', True),
                'service_name': (booking_obj.get('service_name') or
                                booking_obj.get('service') or
                                booking_obj.get('title')),
                'booking_type': booking_obj.get('_type'),  # From JSON API format
                'extracted_at': datetime.now().isoformat()
            }

            # DEBUG: Log what we actually parsed
            logger.info(f"ðŸ” [DEBUG] Parsed result: date={result.get('date')}, time={result.get('time')}, datetime_str={datetime_str[:30] if datetime_str else 'None'}")

            # Only return if we have required fields (BOTH date AND time)
            if result['date'] and result['time']:
                logger.info(f"âœ… [API-PARSE] Parsed booking: date={result['date']}, time={result['time']}, price={result['price']}, type={result.get('booking_type')}")
                return result
            else:
                logger.warning(f"âš ï¸ [API-PARSE] Skipping object without date/time: {str(booking_obj)[:150]}")

        except Exception as e:
            logger.warning(f"âŒ [API-PARSE] Failed to parse booking object: {e} | Data: {str(booking_obj)[:150]}")

        return None

    async def detect_and_handle_page_type(self, page: Page, original_url: str, current_url: str) -> List[Dict]:
        """
        Smart detection of YClients page type and routing to appropriate handler.

        Handles:
        - City/branch selection pages (redirected multi-location venues)
        - Menu pages (/personal/menu)
        - Time selection pages (/personal/select-time - mid-flow)
        - Standard record-type flow
        """
        try:
            page_title = await page.title()
            logger.info(f"ðŸ” [DETECTION] Page title: {page_title}")

            # Check for city/branch selection redirect
            if '/select-city' in current_url or '/select-branch' in current_url:
                logger.warning(f"âš ï¸ [DETECTION] Redirected to city/branch selection page")
                return await self.handle_multi_location_redirect(page, original_url)

            # Check if on menu page
            elif '/personal/menu' in current_url:
                logger.info(f"âœ… [DETECTION] Menu page detected")
                return await self.handle_menu_page(page, current_url)

            # Check if already at time selection (mid-flow URL)
            elif '/personal/select-time' in current_url:
                logger.info(f"âœ… [DETECTION] Time selection page (mid-flow entry)")
                return await self.handle_time_selection_page(page, current_url)

            # Standard flow (record-type or similar)
            else:
                logger.info(f"âœ… [DETECTION] Standard booking flow page")
                return await self.navigate_yclients_flow(page, original_url)

        except Exception as e:
            logger.error(f"âŒ [DETECTION] Error in page type detection: {str(e)}")
            # Fallback to standard flow
            return await self.navigate_yclients_flow(page, original_url)

    async def handle_multi_location_redirect(self, page: Page, original_url: str) -> List[Dict]:
        """
        Handle pages that redirect to city/branch selection.
        Try to select first available location and continue.
        """
        logger.info("ðŸ¢ [MULTI-LOC] Attempting to handle multi-location redirect")

        try:
            # Wait for page to fully load
            await page.wait_for_timeout(3000)

            # CRITICAL FIX: Use div with hasText filter to find location cards
            # Based on Playwright exploration findings - branch selection uses nested divs
            try:
                # Look for location names in the page
                location_patterns = [
                    'Lunda Padel',
                    'Padel',
                    'Ñ„Ð¸Ð»Ð¸Ð°Ð»',  # Branch in Russian
                ]

                for pattern in location_patterns:
                    try:
                        # Find clickable divs containing location names
                        locations = await page.locator(f'div[cursor="pointer"]:has-text("{pattern}")').all()

                        # Alternative: Find any clickable generic elements with location text
                        if not locations:
                            locations = await page.locator(f'generic[cursor="pointer"]:has-text("{pattern}")').all()

                        # Fallback: Use JavaScript to find clickable elements with text content
                        if not locations:
                            locations = await page.evaluate(f'''() => {{
                                const pattern = "{pattern}";
                                const clickable = [];
                                const allDivs = document.querySelectorAll('div, generic');

                                allDivs.forEach(div => {{
                                    const style = window.getComputedStyle(div);
                                    const text = div.textContent || '';

                                    if (text.includes(pattern) &&
                                        style.cursor === 'pointer' &&
                                        div.offsetHeight > 0) {{
                                        clickable.push(div);
                                    }}
                                }});

                                return clickable;
                            }}''')

                            if locations and len(locations) > 0:
                                logger.info(f"ðŸ¢ [MULTI-LOC] Found {len(locations)} clickable locations via JS with pattern '{pattern}'")
                                # Click first one using JavaScript
                                await page.evaluate('(el) => el.click()', locations[0])
                                await page.wait_for_load_state('networkidle', timeout=10000)

                                new_url = page.url
                                logger.info(f"ðŸ¢ [MULTI-LOC] After JS click, new URL: {new_url}")
                                return await self.detect_and_handle_page_type(page, original_url, new_url)

                        if locations and len(locations) > 0:
                            logger.info(f"ðŸ¢ [MULTI-LOC] Found {len(locations)} clickable locations with pattern '{pattern}'")

                            # Click first available location
                            first_location = locations[0]
                            location_text = await first_location.text_content()
                            logger.info(f"ðŸ¢ [MULTI-LOC] Clicking first location: {location_text[:100]}")

                            await first_location.click(force=True, timeout=5000)
                            await page.wait_for_load_state('networkidle', timeout=10000)

                            new_url = page.url
                            logger.info(f"ðŸ¢ [MULTI-LOC] After click, new URL: {new_url}")
                            return await self.detect_and_handle_page_type(page, original_url, new_url)

                    except Exception as e:
                        logger.debug(f"ðŸ¢ [MULTI-LOC] Pattern '{pattern}' search failed: {e}")
                        continue

            except Exception as e:
                logger.warning(f"ðŸ¢ [MULTI-LOC] Advanced location search failed: {e}")

            # Fallback to old selectors
            branch_selectors = [
                'div[cursor="pointer"]',    # Generic clickable divs
                'ui-kit-simple-cell',       # YClients UI cells
                'a[href*="/company/"]',     # Links to specific company pages
                'a[href*="record-type"]',   # Direct booking links
            ]

            for selector in branch_selectors:
                try:
                    elements = await page.locator(selector).all()
                    if elements and len(elements) > 0:
                        logger.info(f"ðŸ¢ [MULTI-LOC] Found {len(elements)} elements with selector: {selector}")

                        # Click first location (use force for Angular components)
                        first_element = elements[0]
                        element_text = await first_element.text_content()
                        logger.info(f"ðŸ¢ [MULTI-LOC] Clicking first location: {element_text[:50]}")

                        await first_element.click(force=True, timeout=5000)
                        await page.wait_for_load_state('networkidle', timeout=10000)

                        # Now recursively detect the new page type
                        new_url = page.url
                        logger.info(f"ðŸ¢ [MULTI-LOC] After click, new URL: {new_url}")
                        return await self.detect_and_handle_page_type(page, original_url, new_url)

                except Exception as e:
                    logger.debug(f"ðŸ¢ [MULTI-LOC] Selector {selector} failed: {e}")
                    continue

            # If no location links found, cannot proceed
            logger.warning(f"âš ï¸ [MULTI-LOC] No location links found, cannot select branch")
            logger.info(f"ðŸ¢ [MULTI-LOC] Page HTML snippet: {(await page.content())[:500]}")
            return []

        except Exception as e:
            logger.error(f"âŒ [MULTI-LOC] Error handling multi-location: {str(e)}")
            return []

    async def handle_menu_page(self, page: Page, url: str) -> List[Dict]:
        """
        Handle /personal/menu pages where services are listed but as menu items.
        Extract available services and navigate to each.
        """
        logger.info("ðŸ“‹ [MENU] Extracting services from menu page")

        results = []
        try:
            # Menu pages typically have service cards/cells
            # Try to find clickable service elements
            service_selectors = [
                'ui-kit-simple-cell',
                '[class*="service"]',
                'a[href*="select-time"]',
                '.menu-item',
            ]

            for selector in service_selectors:
                try:
                    await page.wait_for_selector(selector, timeout=5000)
                    services = await page.locator(selector).all()

                    if services:
                        logger.info(f"ðŸ“‹ [MENU] Found {len(services)} services with selector: {selector}")

                        # Click on first few services to get their booking flows
                        for i, service in enumerate(services[:3]):  # Limit to 3 services
                            try:
                                service_text = await service.text_content()
                                logger.info(f"ðŸ“‹ [MENU] Clicking service {i+1}: {service_text[:30]}")

                                await service.click()
                                await page.wait_for_load_state('networkidle', timeout=5000)

                                # Now we should be in booking flow - detect and continue
                                current_url = page.url
                                service_results = await self.detect_and_handle_page_type(page, url, current_url)
                                results.extend(service_results)

                                # Go back to menu
                                await page.go_back()
                                await page.wait_for_load_state('networkidle', timeout=5000)

                            except Exception as e:
                                logger.warning(f"âš ï¸ [MENU] Failed to process service {i+1}: {e}")
                                continue

                        break  # Found services, stop trying other selectors

                except Exception as e:
                    logger.debug(f"ðŸ“‹ [MENU] Selector {selector} failed: {e}")
                    continue

            return results

        except Exception as e:
            logger.error(f"âŒ [MENU] Error handling menu page: {str(e)}")
            return []

    async def handle_time_selection_page(self, page: Page, url: str) -> List[Dict]:
        """
        Handle pages that start directly at time selection (/personal/select-time).
        CRITICAL FIX: Now navigates full flow TIME â†’ COURT â†’ SERVICE to capture ALL data.

        Real YClients flow (confirmed from Playwright exploration):
        1. TIME selection (this page) - capture dates/times
        2. COURT selection - capture court names (DOM scrape)
        3. SERVICE selection - capture prices (DOM scrape)
        """
        logger.info("â° [TIME-PAGE] Starting FULL flow navigation (TIME â†’ COURT â†’ SERVICE)")

        results = []
        scraped_data = {'dates': [], 'times': [], 'courts': [], 'services': []}

        try:
            # Wait for time selection elements
            await page.wait_for_timeout(2000)  # Let page fully load

            # Check if page shows "No free time" message with "Go to nearest date" button
            try:
                nearest_date_btn = page.get_by_role('button', name=re.compile(r'ÐŸÐµÑ€ÐµÐ¹Ñ‚Ð¸.*Ð±Ð»Ð¸Ð¶Ð°Ð¹ÑˆÐµÐ¹.*Ð´Ð°Ñ‚Ðµ'))
                if await nearest_date_btn.is_visible(timeout=2000):
                    logger.info("â° [TIME-PAGE] Found 'Go to nearest date' button, clicking...")
                    await nearest_date_btn.click(force=True)
                    await page.wait_for_timeout(3000)  # Wait for time slots to appear

                    # Time slots should now be visible - proceed to click one
                    try:
                        # Look for time slots (format: "9:00", "22:00", etc.)
                        time_slots = await page.get_by_text(re.compile(r'^\d{1,2}:\d{2}$')).all()
                        if not time_slots:
                            raise Exception("No time slots found")
                        time_slot = time_slots[0]
                        time_text = await time_slot.text_content()
                        logger.info(f"â° [TIME-PAGE] Clicking time slot: {time_text}")

                        await time_slot.click(force=True)
                        await page.wait_for_timeout(1500)

                        # Click ÐŸÑ€Ð¾Ð´Ð¾Ð»Ð¶Ð¸Ñ‚ÑŒ button
                        continue_btn = page.get_by_role('button', name='ÐŸÑ€Ð¾Ð´Ð¾Ð»Ð¶Ð¸Ñ‚ÑŒ')
                        if await continue_btn.is_visible(timeout=2000):
                            logger.info("ðŸŽ¯ Clicking ÐŸÑ€Ð¾Ð´Ð¾Ð»Ð¶Ð¸Ñ‚ÑŒ")
                            await continue_btn.click(force=True)
                            await page.wait_for_load_state('networkidle', timeout=10000)

                            # Should now be on select-services page
                            if 'select-services' in page.url:
                                logger.info("âœ… [FLOW-A] On service page - scraping prices")

                                # Get provider (court name) - try multiple page structures
                                provider = 'Unknown'
                                provider_selectors = [
                                    'paragraph',                     # Structure A (b861100 - Angular paragraph element)
                                    'p.label.category-title',       # Structure A alternative
                                    'div.header_title',             # Structure B (b1009933 - TK Raketion)
                                    'div.title-block__title',       # Structure C (alternative)
                                    'h1.category-title',            # Structure D (fallback)
                                    '.service-category-title',      # Structure E (fallback)
                                ]

                                for selector in provider_selectors:
                                    try:
                                        provider_el = page.locator(selector).first
                                        provider_text = await provider_el.text_content(timeout=2000)
                                        if provider_text and provider_text.strip():
                                            provider = provider_text.strip()
                                            logger.info(f"ðŸŸï¸ Provider found with selector '{selector}': {provider}")
                                            break
                                    except Exception:
                                        continue

                                if provider == 'Unknown':
                                    logger.warning(f"âš ï¸ Failed to get provider: No matching selector found")

                                # Get prices (text with â‚½ symbol)
                                try:
                                    price_elements = await page.get_by_text(re.compile(r'\d+[,\s]*\d*\s*â‚½')).all()
                                    logger.info(f"ðŸ’° Found {len(price_elements)} prices")

                                    # Get date (from button click - it's the suggested date)
                                    from datetime import timedelta
                                    suggested_date = (datetime.now() + timedelta(days=2)).strftime('%Y-%m-%d')

                                    for price_el in price_elements:
                                        price_text = await price_el.text_content()
                                        price_clean = price_text.strip()

                                        result = {
                                            'url': page.url,
                                            'date': suggested_date,
                                            'time': time_text.strip(),
                                            'provider': provider,
                                            'price': price_clean,
                                            'service_name': 'Court Rental',
                                            'duration': 60,
                                            'available': True,
                                            'extracted_at': datetime.now().isoformat()
                                        }
                                        results.append(result)
                                        logger.info(f"âœ… [PRODUCTION-PROOF] PRICE CAPTURED: {price_clean}")
                                        logger.info(f"âœ… [PRODUCTION-PROOF] Full record: date={suggested_date}, time={time_text.strip()}, provider={provider}, price={price_clean}")

                                    # Return early with results!
                                    if results:
                                        logger.info(f"âœ… [TIME-PAGE] Extracted {len(results)} records from nearest date")
                                        return results

                                except Exception as e:
                                    logger.error(f"âŒ Failed to scrape prices: {e}")
                    except Exception as e:
                        logger.warning(f"âš ï¸ Failed to process nearest date: {e}")
            except:
                pass  # Button not found, continue with date selection

            # Check if time slots are already visible (after clicking nearest date button)
            try:
                # Wait a bit for DOM to update after clicking button
                await page.wait_for_timeout(1000)

                # Try to find time slots using pattern matching
                # Time slots contain text like "9:00", "10:30" etc.
                # Use partial match since elements may have whitespace
                time_slot_candidates = await page.get_by_text(re.compile(r'\d{1,2}:\d{2}')).all()

                # Filter to only actual time slots (not other elements with colons)
                time_slots = []
                for candidate in time_slot_candidates:
                    text = await candidate.text_content()
                    text_clean = text.strip() if text else ''
                    # Check if it matches time format (HH:MM)
                    if re.match(r'^\d{1,2}:\d{2}$', text_clean):
                        time_slots.append(candidate)

                if len(time_slots) > 0:
                    logger.info(f"â° [TIME-PAGE] Time slots already visible, found {len(time_slots)} slots")
                    logger.info("â° [TIME-PAGE] Extracting directly without clicking dates...")

                    # Get the current date (shown on page)
                    try:
                        # Try to find selected/highlighted date
                        selected_date_text = await page.locator('.calendar-day.selected, .calendar-day.active, [class*="selected"][class*="date"]').first.text_content()
                        parsed_date = self.parse_date(selected_date_text)
                    except:
                        parsed_date = datetime.now().strftime('%Y-%m-%d')

                    logger.info(f"â° [TIME-PAGE] Current date: {parsed_date}")

                    # Process each visible time slot
                    for slot_idx, slot in enumerate(time_slots[:3]):
                        try:
                            time_text = await slot.text_content()
                            time_clean = time_text.strip() if time_text else ''
                            logger.info(f"â° [STEP-2] Clicking time slot: {time_clean}")

                            await slot.click(force=True, timeout=5000)
                            await page.wait_for_timeout(1500)

                            # Check for "ÐŸÑ€Ð¾Ð´Ð¾Ð»Ð¶Ð¸Ñ‚ÑŒ" button
                            try:
                                continue_btn = page.get_by_role('button', name='ÐŸÑ€Ð¾Ð´Ð¾Ð»Ð¶Ð¸Ñ‚ÑŒ')
                                if await continue_btn.is_visible(timeout=2000):
                                    logger.info(f"ðŸŽ¯ [STEP-2] Clicking 'ÐŸÑ€Ð¾Ð´Ð¾Ð»Ð¶Ð¸Ñ‚ÑŒ' to next page")
                                    await continue_btn.click()
                                    await page.wait_for_load_state('networkidle', timeout=10000)

                                    current_url = page.url
                                    logger.info(f"ðŸ” [STEP-2] Landed on: {current_url}")

                                    # Check if we're on service page
                                    if 'select-services' in current_url:
                                        logger.info(f"âœ… [FLOW-A] Direct service page - scraping prices")

                                        # Get provider/court name
                                        provider_name = 'Unknown'
                                        try:
                                            provider_el = await page.locator('paragraph').first
                                            provider_name = await provider_el.text_content()
                                            provider_name = provider_name.strip() if provider_name else 'Unknown'
                                            logger.info(f"ðŸŸï¸ [FLOW-A] Provider: {provider_name}")
                                        except Exception as e:
                                            logger.warning(f"âš ï¸ [FLOW-A] Failed to get provider: {e}")

                                        # Get all prices
                                        try:
                                            price_elements = await page.get_by_text(re.compile(r'\d+[,\s]*\d*\s*â‚½')).all()
                                            logger.info(f"ðŸ’° [FLOW-A] Found {len(price_elements)} prices")

                                            for idx, price_el in enumerate(price_elements):
                                                try:
                                                    price_text = await price_el.text_content()
                                                    price_clean = price_text.strip() if price_text else None

                                                    if price_clean:
                                                        result = {
                                                            'url': page.url,
                                                            'date': parsed_date,
                                                            'time': time_clean,
                                                            'provider': provider_name,
                                                            'price': price_clean,
                                                            'service_name': 'Unknown Service',
                                                            'duration': 60,
                                                            'available': True,
                                                            'extracted_at': datetime.now().isoformat()
                                                        }
                                                        results.append(result)
                                                        logger.info(f"âœ… [FLOW-A] Scraped: {parsed_date} {time_clean} â†’ {provider_name} â†’ {price_clean}")
                                                except Exception as e:
                                                    logger.warning(f"âš ï¸ [FLOW-A] Failed to extract price {idx+1}: {e}")

                                        except Exception as e:
                                            logger.error(f"âŒ [FLOW-A] Failed to get prices: {e}")

                                        # Go back to time selection
                                        await page.go_back()
                                        await page.wait_for_timeout(1000)

                            except Exception as e:
                                logger.warning(f"âš ï¸ [STEP-2] No 'ÐŸÑ€Ð¾Ð´Ð¾Ð»Ð¶Ð¸Ñ‚ÑŒ' button: {e}")

                        except Exception as e:
                            logger.warning(f"âš ï¸ [STEP-2] Failed to process time slot {slot_idx+1}: {e}")
                            continue

                    # If we got results, return them
                    if results:
                        logger.info(f"âœ… [TIME-PAGE] Extracted {len(results)} records from visible time slots")
                        return results

            except:
                pass  # Time slots not visible, continue with date iteration

            # STEP 1: Extract dates from DOM
            date_selectors = [
                '.calendar-day:not(.disabled)',
                '[class*="date"]',
                '[data-date]',
            ]

            dates_found = []
            for selector in date_selectors:
                try:
                    dates = await page.locator(selector).all()
                    if dates and len(dates) > 0:
                        dates_found = dates
                        logger.info(f"â° [TIME-PAGE] Found {len(dates)} dates with selector: {selector}")
                        break
                except:
                    continue

            if not dates_found:
                logger.warning("âš ï¸ [TIME-PAGE] No dates found on time selection page")
                return []

            # STEP 2: Navigate through dates â†’ times â†’ courts â†’ services
            for date_idx, date in enumerate(dates_found[:2]):  # Limit to 2 dates for performance
                try:
                    date_text = await date.text_content()
                    parsed_date = self.parse_date(date_text)
                    logger.info(f"â° [STEP-1] Processing date {date_idx+1}: {date_text[:20]} â†’ {parsed_date}")

                    # Scroll into view and click date to load time slots
                    await date.scroll_into_view_if_needed()
                    await page.wait_for_timeout(500)
                    await date.click(force=True, timeout=5000)
                    await page.wait_for_timeout(2000)  # Give time for slots to load

                    # Extract time slots for this date
                    time_slots = []
                    time_slot_selectors = [
                        '[data-time]',
                        'button[class*="time"]',
                        '.time-slot',
                        'div[class*="slot"]',
                    ]

                    for selector in time_slot_selectors:
                        try:
                            slots = await page.locator(selector).all()
                            if slots:
                                time_slots = slots
                                logger.info(f"â° [STEP-1] Found {len(time_slots)} time slots with selector: {selector}")
                                break
                        except:
                            continue

                    # Fallback: search for time patterns
                    if not time_slots:
                        time_slots = await page.get_by_text(re.compile(r'\d{1,2}:\d{2}')).all()
                        if time_slots:
                            logger.info(f"â° [STEP-1] Found {len(time_slots)} time slots via text pattern")

                    # Navigate through each time slot
                    for slot_idx, slot in enumerate(time_slots[:3]):  # Limit to 3 slots per date
                        try:
                            time_text = await slot.text_content()
                            time_clean = time_text.strip() if time_text else ''
                            logger.info(f"â° [STEP-2] Clicking time slot: {time_clean}")

                            # Click time slot
                            await slot.click(force=True, timeout=5000)
                            await page.wait_for_timeout(1500)

                            # Check for "ÐŸÑ€Ð¾Ð´Ð¾Ð»Ð¶Ð¸Ñ‚ÑŒ" button to go to next step
                            try:
                                continue_btn = page.get_by_role('button', name='ÐŸÑ€Ð¾Ð´Ð¾Ð»Ð¶Ð¸Ñ‚ÑŒ')
                                if await continue_btn.is_visible(timeout=2000):
                                    logger.info(f"ðŸŽ¯ [STEP-2] Clicking 'ÐŸÑ€Ð¾Ð´Ð¾Ð»Ð¶Ð¸Ñ‚ÑŒ' to next page")
                                    await continue_btn.click()
                                    await page.wait_for_load_state('networkidle', timeout=10000)

                                    # CHECK: Which page did we land on?
                                    current_url = page.url
                                    logger.info(f"ðŸ” [STEP-2] Landed on: {current_url}")

                                    # FLOW A: Direct to SERVICE page (no court selection!)
                                    if 'select-services' in current_url:
                                        logger.info(f"âœ… [FLOW-A] Direct service page detected - scraping prices")

                                        # Get provider/court name from first paragraph
                                        provider_name = 'Unknown'
                                        try:
                                            provider_el = await page.locator('paragraph').first
                                            provider_name = await provider_el.text_content()
                                            provider_name = provider_name.strip() if provider_name else 'Unknown'
                                            logger.info(f"ðŸŸï¸ [FLOW-A] Provider: {provider_name}")
                                        except Exception as e:
                                            logger.warning(f"âš ï¸ [FLOW-A] Failed to get provider: {e}")

                                        # Get all prices from page (they contain â‚½ symbol)
                                        try:
                                            price_elements = await page.get_by_text(re.compile(r'\d+[,\s]*\d*\s*â‚½')).all()
                                            logger.info(f"ðŸ’° [FLOW-A] Found {len(price_elements)} prices")

                                            for idx, price_el in enumerate(price_elements):
                                                try:
                                                    price_text = await price_el.text_content()
                                                    price_clean = price_text.strip() if price_text else None

                                                    if price_clean:
                                                        # Try to get service name (text before price)
                                                        service_name = 'Unknown Service'
                                                        try:
                                                            # Get parent element and extract service text
                                                            parent = await price_el.locator('xpath=ancestor::*[contains(text(), "Ð°Ñ€ÐµÐ½Ð´Ð°")]').first
                                                            service_text = await parent.text_content()
                                                            if service_text and 'Ð°Ñ€ÐµÐ½Ð´Ð°' in service_text:
                                                                service_name = service_text.split('\n')[0].strip()
                                                        except:
                                                            pass

                                                        result = {
                                                            'url': page.url,
                                                            'date': parsed_date,
                                                            'time': time_clean,
                                                            'provider': provider_name,
                                                            'price': price_clean,
                                                            'service_name': service_name,
                                                            'duration': 60,
                                                            'available': True,
                                                            'extracted_at': datetime.now().isoformat()
                                                        }
                                                        results.append(result)
                                                        logger.info(f"âœ… [FLOW-A] Scraped: {parsed_date} {time_clean} â†’ {provider_name} â†’ {price_clean}")
                                                except Exception as e:
                                                    logger.warning(f"âš ï¸ [FLOW-A] Failed to extract price {idx+1}: {e}")

                                        except Exception as e:
                                            logger.error(f"âŒ [FLOW-A] Failed to get prices: {e}")

                                        # Go back to time selection
                                        await page.go_back()
                                        await page.wait_for_timeout(1000)
                                        continue  # Skip court navigation logic

                                    # FLOW B: Court selection page (original multi-step flow)
                                    # STEP 3: Now on court selection page - SCRAPE COURT NAMES
                                    logger.info(f"ðŸŸï¸ [STEP-3] On court selection page, scraping court names")

                                    court_selectors = [
                                        'ui-kit-simple-cell',
                                        '[class*="court"]',
                                        '[class*="staff"]',
                                        '.service-item',
                                    ]

                                    courts_found = []
                                    for selector in court_selectors:
                                        try:
                                            courts = await page.locator(selector).all()
                                            if courts and len(courts) > 0:
                                                courts_found = courts
                                                logger.info(f"ðŸŸï¸ [STEP-3] Found {len(courts)} courts with selector: {selector}")
                                                break
                                        except:
                                            continue

                                    if not courts_found:
                                        logger.warning(f"âš ï¸ [STEP-3] No courts found on page")
                                        # Go back and continue with next time slot
                                        await page.go_back()
                                        await page.wait_for_timeout(1000)
                                        continue

                                    # Navigate through courts
                                    for court_idx, court in enumerate(courts_found[:3]):  # Limit to 3 courts
                                        try:
                                            # Extract court name BEFORE clicking
                                            court_name = 'Unknown'
                                            try:
                                                court_name_el = await court.locator('ui-kit-headline').first
                                                court_name = await court_name_el.text_content()
                                                court_name = court_name.strip() if court_name else 'Unknown'
                                            except:
                                                court_name = await court.text_content()
                                                court_name = court_name[:50].strip() if court_name else 'Unknown'

                                            logger.info(f"ðŸŸï¸ [STEP-4] Clicking court: {court_name}")

                                            # Click court
                                            await court.click(force=True, timeout=5000)
                                            await page.wait_for_timeout(1500)

                                            # Click "ÐŸÑ€Ð¾Ð´Ð¾Ð»Ð¶Ð¸Ñ‚ÑŒ" to go to service/price page
                                            continue_btn2 = page.get_by_role('button', name='ÐŸÑ€Ð¾Ð´Ð¾Ð»Ð¶Ð¸Ñ‚ÑŒ')
                                            if await continue_btn2.is_visible(timeout=2000):
                                                logger.info(f"ðŸŽ¯ [STEP-4] Clicking 'ÐŸÑ€Ð¾Ð´Ð¾Ð»Ð¶Ð¸Ñ‚ÑŒ' to service/price page")
                                                await continue_btn2.click()
                                                await page.wait_for_load_state('networkidle', timeout=10000)

                                                # STEP 4: Now on service/price page - SCRAPE PRICES
                                                logger.info(f"ðŸ’° [STEP-5] On service/price page, scraping prices")

                                                # Extract service items with prices
                                                service_selectors = [
                                                    'ui-kit-simple-cell',
                                                    '[class*="service"]',
                                                    '.price-item',
                                                ]

                                                services_found = []
                                                for selector in service_selectors:
                                                    try:
                                                        services = await page.locator(selector).all()
                                                        if services and len(services) > 0:
                                                            services_found = services
                                                            logger.info(f"ðŸ’° [STEP-5] Found {len(services)} services with selector: {selector}")
                                                            break
                                                    except:
                                                        continue

                                                for svc_idx, service in enumerate(services_found):
                                                    try:
                                                        # Extract service name
                                                        service_name = 'Unknown Service'
                                                        try:
                                                            name_el = await service.locator('ui-kit-headline').first
                                                            service_name = await name_el.text_content()
                                                            service_name = service_name.strip() if service_name else 'Unknown Service'
                                                        except:
                                                            pass

                                                        # Extract price
                                                        price = 'ÐÐµ Ð½Ð°Ð¹Ð´ÐµÐ½Ð°'
                                                        try:
                                                            price_el = await service.locator('ui-kit-title').first
                                                            price = await price_el.text_content()
                                                            price = self.clean_price(price) if price else 'ÐÐµ Ð½Ð°Ð¹Ð´ÐµÐ½Ð°'
                                                        except:
                                                            pass

                                                        # Extract duration
                                                        duration = 60
                                                        try:
                                                            duration_el = await service.locator('ui-kit-body').first
                                                            duration_text = await duration_el.text_content()
                                                            duration = self.parse_duration(duration_text) if duration_text else 60
                                                        except:
                                                            pass

                                                        # Create complete booking record with ALL data
                                                        result = {
                                                            'url': page.url,
                                                            'date': parsed_date,
                                                            'time': time_clean,
                                                            'provider': court_name,
                                                            'price': price,
                                                            'service_name': service_name,
                                                            'duration': duration,
                                                            'available': True,
                                                            'extracted_at': datetime.now().isoformat()
                                                        }
                                                        results.append(result)
                                                        logger.info(f"âœ… [STEP-5] Scraped complete record: date={parsed_date}, time={time_clean}, court={court_name}, price={price}")

                                                    except Exception as e:
                                                        logger.warning(f"âš ï¸ [STEP-5] Failed to extract service {svc_idx+1}: {e}")

                                                # Go back to court selection
                                                await page.go_back()
                                                await page.wait_for_timeout(1000)

                                        except Exception as e:
                                            logger.warning(f"âš ï¸ [STEP-4] Failed to process court {court_idx+1}: {e}")
                                            continue

                                    # Go back to time selection
                                    await page.go_back()
                                    await page.wait_for_timeout(1000)

                            except Exception as e:
                                logger.warning(f"âš ï¸ [STEP-2] No 'ÐŸÑ€Ð¾Ð´Ð¾Ð»Ð¶Ð¸Ñ‚ÑŒ' button or navigation failed: {e}")

                        except Exception as e:
                            logger.warning(f"âš ï¸ [STEP-2] Failed to process time slot {slot_idx+1}: {e}")
                            continue

                except Exception as e:
                    logger.warning(f"âš ï¸ [STEP-1] Failed to process date {date_idx+1}: {e}")
                    continue

            logger.info(f"âœ… [TIME-PAGE] Complete flow navigation finished: {len(results)} records extracted")
            return results

        except Exception as e:
            logger.error(f"âŒ [TIME-PAGE] Error handling time selection page: {str(e)}")
            return []

    async def navigate_yclients_flow(self, page: Page, url: str) -> List[Dict]:
        """
        Navigate through YClients 4-step booking flow.
        Step 1: Service selection (record-type)
        Step 2: Court selection (select-master)
        Step 3: Date/time selection (select-time)
        Step 4: Service packages with prices (select-services)
        """
        results = []
        logger.info(f"ðŸ” [DEBUG] Starting 4-step YClients navigation for {url}")

        try:
            # Step 1: Load and select service type
            logger.info(f"ðŸ” [DEBUG] Step 1: Loading page and waiting for ui-kit-simple-cell")
            await page.goto(url, wait_until='networkidle')
            await page.wait_for_selector('ui-kit-simple-cell', timeout=10000)
            logger.info(f"ðŸ” [DEBUG] Step 1: Page loaded, title: {await page.title()}")
            
            # Click on "Ð˜Ð½Ð´Ð¸Ð²Ð¸Ð´ÑƒÐ°Ð»ÑŒÐ½Ñ‹Ðµ ÑƒÑÐ»ÑƒÐ³Ð¸" or first available service
            service_links = await page.get_by_role('link').all()
            logger.info(f"ðŸ” [DEBUG] Step 1: Found {len(service_links)} service links")
            service_clicked = False
            for link in service_links:
                text = await link.text_content()
                if 'Ð˜Ð½Ð´Ð¸Ð²Ð¸Ð´ÑƒÐ°Ð»ÑŒÐ½Ñ‹Ðµ' in text or 'ÑƒÑÐ»ÑƒÐ³Ð¸' in text.lower():
                    logger.info(f"ðŸ” [DEBUG] Step 1: Clicking service link: {text[:50]}")
                    await link.click()
                    service_clicked = True
                    break

            if not service_clicked:
                logger.warning(f"âš ï¸ [DEBUG] Step 1: No matching service link found, trying first link")
                if service_links:
                    await service_links[0].click()

            # Step 2: Select courts
            logger.info(f"ðŸ” [DEBUG] Step 2: Waiting for select-master page")
            await page.wait_for_url('**/personal/select-master**')
            await page.wait_for_selector('ui-kit-simple-cell')
            logger.info(f"ðŸ” [DEBUG] Step 2: On select-master page")

            courts = await page.locator('ui-kit-simple-cell').all()
            logger.info(f"ðŸ” [DEBUG] Step 2: Found {len(courts)} courts")
            for i, court in enumerate(courts[:3]):  # Limit to first 3 courts for testing
                court_name = await court.locator('ui-kit-headline').text_content()
                logger.info(f"ðŸ” [DEBUG] Step 2: Processing court {i+1}/3: {court_name[:50]}")
                await court.click()

                # Continue to date selection
                continue_btn = page.get_by_role('button', { 'name': 'ÐŸÑ€Ð¾Ð´Ð¾Ð»Ð¶Ð¸Ñ‚ÑŒ' })
                if await continue_btn.is_visible():
                    logger.info(f"ðŸ” [DEBUG] Step 2: Clicking 'ÐŸÑ€Ð¾Ð´Ð¾Ð»Ð¶Ð¸Ñ‚ÑŒ' button")
                    await continue_btn.click()

                # Step 3: Select dates and times
                logger.info(f"ðŸ” [DEBUG] Step 3: Waiting for select-time page")
                await page.wait_for_url('**/personal/select-time**')
                logger.info(f"ðŸ” [DEBUG] Step 3: Extracting time slots for {court_name[:30]}")
                before_count = len(results)
                await self.extract_time_slots_with_prices(page, court_name, results)
                after_count = len(results)
                logger.info(f"ðŸ” [DEBUG] Step 3: Extracted {after_count - before_count} slots for this court")

                # Go back to court selection
                await page.go_back()
                await page.wait_for_selector('ui-kit-simple-cell')

        except Exception as e:
            logger.error(f"âŒ [DEBUG] Error in 4-step navigation: {str(e)}")
            logger.error(f"âŒ [DEBUG] Current URL: {page.url}")
            logger.error(f"âŒ [DEBUG] Page title: {await page.title()}")

        logger.info(f"ðŸ” [DEBUG] Navigation complete: extracted {len(results)} total results")
        if not results:
            logger.warning(f"âš ï¸ [DEBUG] ZERO results extracted! This needs investigation.")

        return results

    async def extract_time_slots_with_prices(self, page: Page, court_name: str, results: List[Dict]):
        """Extract time slots and navigate to get prices."""
        logger.info(f"ðŸ” [DEBUG] extract_time_slots_with_prices: Starting for court {court_name[:30]}")

        try:
            # Get available dates
            dates = await page.locator('.calendar-day:not(.disabled)').all()
            logger.info(f"ðŸ” [DEBUG] Found {len(dates)} available dates")

            for date_idx, date in enumerate(dates[:2]):  # Limit to 2 dates for testing
                date_text = await date.text_content()
                logger.info(f"ðŸ” [DEBUG] Processing date {date_idx+1}/2: {date_text[:20]}")
                await date.click()
                await page.wait_for_timeout(1000)

                # Get time slots
                time_slots = await page.locator('[data-time]').all()
                if not time_slots:
                    # Try alternative selector
                    logger.warning(f"âš ï¸ [DEBUG] No [data-time] slots found, trying text regex")
                    time_slots = await page.get_by_text(re.compile(r'\d{1,2}:\d{2}')).all()

                logger.info(f"ðŸ” [DEBUG] Found {len(time_slots)} time slots for this date")

                for slot_idx, slot in enumerate(time_slots[:3]):  # Limit to 3 slots per date
                    time_text = await slot.text_content()
                    logger.info(f"ðŸ” [DEBUG] Processing time slot {slot_idx+1}/3: {time_text[:10]}")
                    await slot.click()

                    # Continue to services/prices
                    continue_btn = page.get_by_role('button', { 'name': 'ÐŸÑ€Ð¾Ð´Ð¾Ð»Ð¶Ð¸Ñ‚ÑŒ' })
                    if await continue_btn.is_visible():
                        await continue_btn.click()

                        # Step 4: Extract prices from service packages
                        logger.info(f"ðŸ” [DEBUG] Step 4: Waiting for select-services page")
                        await page.wait_for_url('**/personal/select-services**')
                        await page.wait_for_selector('ui-kit-simple-cell')
                        logger.info(f"ðŸ” [DEBUG] Step 4: On select-services page")

                        services = await page.locator('ui-kit-simple-cell').all()
                        logger.info(f"ðŸ” [DEBUG] Step 4: Found {len(services)} services")
                        for svc_idx, service in enumerate(services):
                            try:
                                name = await service.locator('ui-kit-headline').text_content()
                                price = await service.locator('ui-kit-title').text_content()
                                duration = await service.locator('ui-kit-body').text_content()

                                # Clean and structure data
                                result = {
                                    'url': page.url,
                                    'court_name': court_name.strip() if court_name else '',
                                    'date': self.parse_date(date_text),
                                    'time': time_text.strip() if time_text else '',
                                    'service_name': name.strip() if name else '',
                                    'price': self.clean_price(price),
                                    'duration': self.parse_duration(duration),
                                    'venue_name': self.extract_venue_name(page.url),
                                    'extracted_at': datetime.now().isoformat()
                                }
                                results.append(result)
                                logger.info(f"ðŸ” [DEBUG] Step 4: Extracted service {svc_idx+1}: {name[:30]} - {price}")
                            except Exception as e:
                                logger.warning(f"âš ï¸ [DEBUG] Failed to extract service {svc_idx+1}: {e}")

                        # Go back to time selection
                        await page.go_back()
                        await page.wait_for_timeout(1000)
        except Exception as e:
            logger.error(f"âŒ [DEBUG] Error extracting time slots with prices: {str(e)}")
            logger.error(f"âŒ [DEBUG] Current URL when error occurred: {page.url}")

    def clean_price(self, price_text: str) -> str:
        """Clean price text: '6,000 â‚½' -> '6000 â‚½'"""
        if not price_text:
            return "Ð¦ÐµÐ½Ð° Ð½Ðµ ÑƒÐºÐ°Ð·Ð°Ð½Ð°"
        # Remove spaces and commas from numbers
        cleaned = re.sub(r'(\d),(\d)', r'\1\2', price_text)
        cleaned = re.sub(r'(\d)\s+(\d)', r'\1\2', cleaned)
        cleaned = cleaned.strip()
        return cleaned if 'â‚½' in cleaned or 'Ñ€ÑƒÐ±' in cleaned else f"{cleaned} â‚½"

    def parse_duration(self, duration_text: str) -> int:
        """Parse duration: '1 Ñ‡ 30 Ð¼Ð¸Ð½' -> 90"""
        if not duration_text:
            return 60
        
        total_minutes = 0
        # Extract hours
        hour_match = re.search(r'(\d+)\s*Ñ‡', duration_text)
        if hour_match:
            total_minutes += int(hour_match.group(1)) * 60
        
        # Extract minutes
        min_match = re.search(r'(\d+)\s*Ð¼Ð¸Ð½', duration_text)
        if min_match:
            total_minutes += int(min_match.group(1))
        
        return total_minutes if total_minutes > 0 else 60

    def parse_date(self, date_text: str) -> str:
        """Parse date from calendar text to ISO format."""
        # For now, return current date. Can be enhanced with proper date parsing
        # Russian month mapping
        months = {
            'ÑÐ½Ð²Ð°Ñ€': '01', 'Ñ„ÐµÐ²Ñ€Ð°Ð»': '02', 'Ð¼Ð°Ñ€Ñ‚': '03', 'Ð°Ð¿Ñ€ÐµÐ»': '04',
            'Ð¼Ð°Ð¹': '05', 'Ð¼Ð°Ð¹': '05', 'Ð¸ÑŽÐ½': '06', 'Ð¸ÑŽÐ»': '07',
            'Ð°Ð²Ð³ÑƒÑÑ‚': '08', 'ÑÐµÐ½Ñ‚ÑÐ±Ñ€': '09', 'Ð¾ÐºÑ‚ÑÐ±Ñ€': '10',
            'Ð½Ð¾ÑÐ±Ñ€': '11', 'Ð´ÐµÐºÐ°Ð±Ñ€': '12'
        }
        
        try:
            # Try to extract day and month
            day_match = re.search(r'(\d{1,2})', date_text)
            if day_match:
                day = day_match.group(1).zfill(2)
                # Find month
                for month_name, month_num in months.items():
                    if month_name in date_text.lower():
                        year = datetime.now().year
                        return f"{year}-{month_num}-{day}"
        except:
            pass
        
        return datetime.now().strftime('%Y-%m-%d')

    def extract_venue_name(self, url: str) -> str:
        """Extract venue name from URL or page content."""
        # This is a placeholder - actual implementation would extract from page
        if 'n1165596' in url:
            return 'ÐÐ°Ð³Ð°Ñ‚Ð¸Ð½ÑÐºÐ°Ñ'
        elif 'n1308467' in url:
            return 'ÐšÐ¾Ñ€Ñ‚Ñ‹-Ð¡ÐµÑ‚ÐºÐ¸'
        elif 'b861100' in url:
            return 'Padel Friends'
        elif 'b1009933' in url:
            return 'Ð¢Ðš Ð Ð°ÐºÐµÑ‚Ð»Ð¾Ð½'
        elif 'b918666' in url:
            return 'Padel A33'
        return 'Unknown Venue'

    async def extract_available_dates(self) -> List[Dict[str, Any]]:
        """
        Ð˜Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ðµ Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ñ‹Ñ… Ð´Ð°Ñ‚ Ð±Ñ€Ð¾Ð½Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ.
        
        Returns:
            List[Dict[str, Any]]: Ð¡Ð¿Ð¸ÑÐ¾Ðº Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ñ‹Ñ… Ð´Ð°Ñ‚
        """
        logger.info("Ð˜Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ðµ Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ñ‹Ñ… Ð´Ð°Ñ‚ Ð±Ñ€Ð¾Ð½Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ")
        try:
            # ÐžÐ¶Ð¸Ð´Ð°ÐµÐ¼ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸ ÐºÐ°Ð»ÐµÐ½Ð´Ð°Ñ€Ñ
            await self.page.wait_for_selector(YCLIENTS_REAL_SELECTORS["calendar"]["calendar_container"], timeout=TIMEOUT)
            
            # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ ÑÐ»ÐµÐ¼ÐµÐ½Ñ‚Ñ‹ Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ñ‹Ñ… Ð´Ð°Ñ‚
            date_elements = await self.page.query_selector_all(YCLIENTS_REAL_SELECTORS["calendar"]["available_dates"])
            
            # Ð˜Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ðµ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð¸Ð· ÑÐ»ÐµÐ¼ÐµÐ½Ñ‚Ð¾Ð²
            available_dates = []
            for date_element in date_elements:
                # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ Ð°Ñ‚Ñ€Ð¸Ð±ÑƒÑ‚Ñ‹, Ñ‚ÐµÐºÑÑ‚ Ð¸ Ð´Ñ€ÑƒÐ³Ð¸Ðµ Ð´Ð°Ð½Ð½Ñ‹Ðµ ÑÐ»ÐµÐ¼ÐµÐ½Ñ‚Ð°
                date_text = await date_element.text_content()
                date_attr = await date_element.get_attribute("data-date")
                
                if date_text and date_attr:
                    available_dates.append({
                        "date": date_attr,
                        "display_text": date_text.strip()
                    })
            
            logger.info(f"ÐÐ°Ð¹Ð´ÐµÐ½Ð¾ {len(available_dates)} Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ñ‹Ñ… Ð´Ð°Ñ‚")
            return available_dates
        except Exception as e:
            logger.error(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ Ð¸Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ð¸ Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ñ‹Ñ… Ð´Ð°Ñ‚: {str(e)}")
            return []

    async def extract_time_slots(self, date: str) -> List[Dict[str, Any]]:
        """
        Ð˜Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ðµ Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ñ‹Ñ… Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ñ… ÑÐ»Ð¾Ñ‚Ð¾Ð² Ð´Ð»Ñ Ð²Ñ‹Ð±Ñ€Ð°Ð½Ð½Ð¾Ð¹ Ð´Ð°Ñ‚Ñ‹.
        
        Args:
            date: Ð”Ð°Ñ‚Ð° Ð´Ð»Ñ Ð²Ñ‹Ð±Ð¾Ñ€Ð°
            
        Returns:
            List[Dict[str, Any]]: Ð¡Ð¿Ð¸ÑÐ¾Ðº Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ñ‹Ñ… Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ñ… ÑÐ»Ð¾Ñ‚Ð¾Ð²
        """
        logger.info(f"Ð˜Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ðµ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ñ… ÑÐ»Ð¾Ñ‚Ð¾Ð² Ð´Ð»Ñ Ð´Ð°Ñ‚Ñ‹: {date}")
        try:
            # Ð’Ñ‹Ð±Ð¸Ñ€Ð°ÐµÐ¼ Ð´Ð°Ñ‚Ñƒ Ð² ÐºÐ°Ð»ÐµÐ½Ð´Ð°Ñ€Ðµ
            date_selector = YCLIENTS_REAL_SELECTORS["calendar"]["date_selector"].format(date=date)
            date_element = await self.page.query_selector(date_selector)
            
            if not date_element:
                logger.warning(f"Ð­Ð»ÐµÐ¼ÐµÐ½Ñ‚ Ð´Ð°Ñ‚Ñ‹ {date} Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½")
                return []
                
            # ÐšÐ»Ð¸ÐºÐ°ÐµÐ¼ Ð½Ð° Ð´Ð°Ñ‚Ñƒ Ð´Ð»Ñ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸ Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ñ‹Ñ… ÑÐ»Ð¾Ñ‚Ð¾Ð²
            await date_element.click()
            await asyncio.sleep(2)  # ÐžÐ¶Ð¸Ð´Ð°Ð½Ð¸Ðµ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸ ÑÐ»Ð¾Ñ‚Ð¾Ð²
            
            # Ð–Ð´ÐµÐ¼ Ð¿Ð¾ÑÐ²Ð»ÐµÐ½Ð¸Ñ ÐºÐ¾Ð½Ñ‚ÐµÐ¹Ð½ÐµÑ€Ð° ÑÐ¾ ÑÐ»Ð¾Ñ‚Ð°Ð¼Ð¸
            await self.page.wait_for_selector(YCLIENTS_REAL_SELECTORS["time_slots"]["container"], timeout=TIMEOUT)
            
            # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ ÑÐ»ÐµÐ¼ÐµÐ½Ñ‚Ñ‹ Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ñ‹Ñ… Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ñ… ÑÐ»Ð¾Ñ‚Ð¾Ð²
            slot_elements = await self.page.query_selector_all(YCLIENTS_REAL_SELECTORS["time_slots"]["slots"])
            
            time_slots = []
            for slot_element in slot_elements:
                # ÐžÐ¿Ñ€ÐµÐ´ÐµÐ»ÑÐµÐ¼, ÑÐ²Ð»ÑÐµÑ‚ÑÑ Ð»Ð¸ Ð´Ð°Ñ‚Ð° Ð²Ñ‹Ñ…Ð¾Ð´Ð½Ñ‹Ð¼ Ð´Ð½ÐµÐ¼
                date_obj = datetime.strptime(date, "%Y-%m-%d")
                is_weekend = date_obj.weekday() >= 5  # 5 Ð¸ 6 - ÑÑƒÐ±Ð±Ð¾Ñ‚Ð° Ð¸ Ð²Ð¾ÑÐºÑ€ÐµÑÐµÐ½ÑŒÐµ
                
                # Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ Ð¸ÑÐ¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð½Ñ‹Ð¹ ÑÐºÑÑ‚Ñ€Ð°ÐºÑ‚Ð¾Ñ€ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð´Ð»Ñ Ð¿Ð¾Ð»ÑƒÑ‡ÐµÐ½Ð¸Ñ Ð²ÑÐµÑ… Ð¿Ð¾Ð»ÐµÐ¹
                slot_data = await self.data_extractor.extract_slot_data_fixed(
                    slot_element
                )
                
                # Ð”Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ Ð´Ð°Ñ‚Ñƒ, ÐµÑÐ»Ð¸ ÐµÑ‘ Ð½ÐµÑ‚
                if "date" not in slot_data:
                    slot_data["date"] = date
                    
                time_slots.append(slot_data)
            
            logger.info(f"ÐÐ°Ð¹Ð´ÐµÐ½Ð¾ {len(time_slots)} Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ñ… ÑÐ»Ð¾Ñ‚Ð¾Ð² Ð´Ð»Ñ Ð´Ð°Ñ‚Ñ‹ {date}")
            return time_slots
            
        except Exception as e:
            logger.error(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ Ð¸Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ð¸ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ñ… ÑÐ»Ð¾Ñ‚Ð¾Ð² Ð´Ð»Ñ Ð´Ð°Ñ‚Ñ‹ {date}: {str(e)}")
            return []

    async def parse_url(self, url: str) -> Tuple[bool, List[Dict[str, Any]]]:
        """
        ÐŸÐ°Ñ€ÑÐ¸Ð½Ð³ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ñ Ð¾Ð´Ð½Ð¾Ð³Ð¾ URL.
        ÐžÐ±Ð½Ð¾Ð²Ð»ÐµÐ½ Ð´Ð»Ñ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸ ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ† Ð²Ñ‹Ð±Ð¾Ñ€Ð° ÑƒÑÐ»ÑƒÐ³ (record-type).
        
        Args:
            url: URL Ð´Ð»Ñ Ð¿Ð°Ñ€ÑÐ¸Ð½Ð³Ð°
            
        Returns:
            Tuple[bool, List[Dict[str, Any]]]: Ð¡Ñ‚Ð°Ñ‚ÑƒÑ ÑƒÑÐ¿ÐµÑ…Ð° Ð¸ ÑÐ¿Ð¸ÑÐ¾Ðº Ð¸Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð½Ñ‹Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ…
        """
        logger.info(f"ÐÐ°Ñ‡Ð°Ð»Ð¾ Ð¿Ð°Ñ€ÑÐ¸Ð½Ð³Ð° URL: {url}")
        all_data = []
        success = False
        
        try:
            # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼, ÑÐ²Ð»ÑÐµÑ‚ÑÑ Ð»Ð¸ ÑÑ‚Ð¾ ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†ÐµÐ¹ Ð²Ñ‹Ð±Ð¾Ñ€Ð° ÑƒÑÐ»ÑƒÐ³
            if 'record-type' in url or 'select-service' in url:
                logger.info("ÐžÐ±Ð½Ð°Ñ€ÑƒÐ¶ÐµÐ½Ð° ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ð° Ð²Ñ‹Ð±Ð¾Ñ€Ð° ÑƒÑÐ»ÑƒÐ³, Ð¿Ð¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ Ð¿Ñ€ÑÐ¼Ñ‹Ðµ ÑÑÑ‹Ð»ÐºÐ¸")
                # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ Ð¿Ñ€ÑÐ¼Ñ‹Ðµ ÑÑÑ‹Ð»ÐºÐ¸ Ð½Ð° ÑƒÑÐ»ÑƒÐ³Ð¸
                direct_urls = await self.handle_service_selection_page(url)
                
                if not direct_urls:
                    logger.warning("ÐÐµ Ð¿Ð¾Ð»ÑƒÑ‡ÐµÐ½Ñ‹ Ð¿Ñ€ÑÐ¼Ñ‹Ðµ ÑÑÑ‹Ð»ÐºÐ¸, Ð¿Ð¾Ð¿Ñ€Ð¾Ð±ÑƒÐµÐ¼ Ð¿Ð°Ñ€ÑÐ¸Ñ‚ÑŒ ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ñƒ ÐºÐ°Ðº ÐµÑÑ‚ÑŒ")
                    # Fallback: Ð¿Ð°Ñ€ÑÐ¸Ð¼ ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ñƒ ÐºÐ°Ðº Ð¾Ð±Ñ‹Ñ‡Ð½Ð¾
                    success, all_data = await self.parse_service_url(url)
                else:
                    # ÐŸÐ°Ñ€ÑÐ¸Ð¼ ÐºÐ°Ð¶Ð´ÑƒÑŽ ÑƒÑÐ»ÑƒÐ³Ñƒ Ð¾Ñ‚Ð´ÐµÐ»ÑŒÐ½Ð¾
                    for service_url in direct_urls:
                        logger.info(f"ÐŸÐ°Ñ€ÑÐ¸Ð½Ð³ ÑƒÑÐ»ÑƒÐ³Ð¸: {service_url}")
                        service_success, service_data = await self.parse_service_url(service_url)
                        if service_success:
                            all_data.extend(service_data)
                            success = True
                        
                        # ÐÐµÐ±Ð¾Ð»ÑŒÑˆÐ°Ñ Ð¿Ð°ÑƒÐ·Ð° Ð¼ÐµÐ¶Ð´Ñƒ Ð·Ð°Ð¿Ñ€Ð¾ÑÐ°Ð¼Ð¸
                        await asyncio.sleep(2)
            else:
                # ÐžÐ±Ñ‹Ñ‡Ð½Ñ‹Ð¹ Ð¿Ð°Ñ€ÑÐ¸Ð½Ð³ Ð¿Ñ€ÑÐ¼Ð¾Ð¹ ÑÑÑ‹Ð»ÐºÐ¸
                success, all_data = await self.parse_service_url(url)
            
            if success:
                self.last_parsed_urls[url] = datetime.now()
                logger.info(f"ÐŸÐ°Ñ€ÑÐ¸Ð½Ð³ URL: {url} Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½ ÑƒÑÐ¿ÐµÑˆÐ½Ð¾, Ð¿Ð¾Ð»ÑƒÑ‡ÐµÐ½Ð¾ {len(all_data)} Ð·Ð°Ð¿Ð¸ÑÐµÐ¹")
            else:
                logger.error(f"ÐŸÐ°Ñ€ÑÐ¸Ð½Ð³ URL: {url} Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½ Ð½ÐµÑƒÐ´Ð°Ñ‡Ð½Ð¾")
            
            return success, all_data
            
        except Exception as e:
            logger.error(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ Ð¿Ð°Ñ€ÑÐ¸Ð½Ð³Ðµ URL {url}: {str(e)}")
            return False, []

    async def parse_service_url(self, url: str) -> Tuple[bool, List[Dict[str, Any]]]:
        """
        ÐŸÐ°Ñ€ÑÐ¸Ð½Ð³ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ñ Ð¿Ñ€ÑÐ¼Ð¾Ð³Ð¾ URL ÑƒÑÐ»ÑƒÐ³Ð¸.
        ÐžÐ±Ð½Ð¾Ð²Ð»ÐµÐ½ Ð´Ð»Ñ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸Ñ 4-ÑˆÐ°Ð³Ð¾Ð²Ð¾Ð³Ð¾ Ð½Ð°Ð²Ð¸Ð³Ð°Ñ†Ð¸Ð¾Ð½Ð½Ð¾Ð³Ð¾ Ð¿Ð¾Ñ‚Ð¾ÐºÐ° YClients.
        
        Args:
            url: URL Ð´Ð»Ñ Ð¿Ð°Ñ€ÑÐ¸Ð½Ð³Ð°
            
        Returns:
            Tuple[bool, List[Dict[str, Any]]]: Ð¡Ñ‚Ð°Ñ‚ÑƒÑ ÑƒÑÐ¿ÐµÑ…Ð° Ð¸ ÑÐ¿Ð¸ÑÐ¾Ðº Ð¸Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð½Ñ‹Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ…
        """
        logger.info(f"ÐŸÐ°Ñ€ÑÐ¸Ð½Ð³ Ð¿Ñ€ÑÐ¼Ð¾Ð¹ ÑÑÑ‹Ð»ÐºÐ¸ ÑƒÑÐ»ÑƒÐ³Ð¸: {url}")
        all_data = []
        
        try:
            # ÐÐ°Ð²Ð¸Ð³Ð°Ñ†Ð¸Ñ Ð½Ð° ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ñƒ
            navigation_success = await self.navigate_to_url(url)
            if not navigation_success:
                logger.error(f"ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ð·Ð°Ð³Ñ€ÑƒÐ·Ð¸Ñ‚ÑŒ ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ñƒ: {url}")
                return False, []
                
            # ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð½Ð° Ð°Ð½Ñ‚Ð¸Ð±Ð¾Ñ‚-Ð·Ð°Ñ‰Ð¸Ñ‚Ñƒ
            if not await self.check_for_antibot():
                logger.warning("ÐžÐ±Ð½Ð°Ñ€ÑƒÐ¶ÐµÐ½Ð° Ð·Ð°Ñ‰Ð¸Ñ‚Ð° Ð¾Ñ‚ Ð±Ð¾Ñ‚Ð¾Ð², ÑÐ¼ÐµÐ½Ð° Ð¿Ñ€Ð¾ÐºÑÐ¸ Ð¸ Ð¿ÐµÑ€ÐµÐ·Ð°Ð¿ÑƒÑÐº")
                return False, []
            
            # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼, ÑÐ²Ð»ÑÐµÑ‚ÑÑ Ð»Ð¸ ÑÑ‚Ð¾ YClients URL
            if self.is_yclients_url(url):
                logger.info("ðŸŽ¯ YClients URL detected, checking page type...")

                # SMART DETECTION: Check what page we actually landed on after navigation
                await self.page.wait_for_load_state('networkidle', timeout=5000)
                current_url = self.page.url
                logger.info(f"ðŸ” [DETECTION] Current URL after load: {current_url}")

                # Try API interception first (best for SPAs), fallback to DOM scraping
                try:
                    logger.info("ðŸŒ [STRATEGY] Attempting API-based extraction first...")
                    all_data = await self.extract_via_api_interception(self.page, url)

                    # If API mode got data, use it
                    if all_data and len(all_data) > 0:
                        logger.info(f"âœ… [STRATEGY] API mode succeeded: {len(all_data)} records")
                    else:
                        # Fallback to DOM scraping
                        logger.info("âš ï¸ [STRATEGY] API mode returned 0 records, falling back to DOM scraping")
                        all_data = await self.detect_and_handle_page_type(self.page, url, current_url)
                except Exception as e:
                    logger.error(f"âŒ [STRATEGY] API mode failed: {e}, falling back to DOM scraping")
                    all_data = await self.detect_and_handle_page_type(self.page, url, current_url)
            else:
                logger.info("ðŸ“„ Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ ÑÑ‚Ð°Ð½Ð´Ð°Ñ€Ñ‚Ð½Ð¾Ðµ Ð¸Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ðµ Ð´Ð°Ð½Ð½Ñ‹Ñ…")
                # Ð˜Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ðµ Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ñ‹Ñ… Ð´Ð°Ñ‚ (ÑÑ‚Ð°Ñ€Ñ‹Ð¹ Ð¼ÐµÑ‚Ð¾Ð´ Ð´Ð»Ñ Ð´Ñ€ÑƒÐ³Ð¸Ñ… ÑÐ°Ð¹Ñ‚Ð¾Ð²)
                available_dates = await self.extract_available_dates()
                if not available_dates:
                    logger.warning("ÐÐµ Ð½Ð°Ð¹Ð´ÐµÐ½Ñ‹ Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ñ‹Ðµ Ð´Ð°Ñ‚Ñ‹")
                    return False, []
                    
                # Ð”Ð»Ñ ÐºÐ°Ð¶Ð´Ð¾Ð¹ Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ð¾Ð¹ Ð´Ð°Ñ‚Ñ‹ Ð¸Ð·Ð²Ð»ÐµÐºÐ°ÐµÐ¼ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ðµ ÑÐ»Ð¾Ñ‚Ñ‹
                for date_info in available_dates:
                    date = date_info["date"]
                    
                    # Ð˜Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ðµ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ñ… ÑÐ»Ð¾Ñ‚Ð¾Ð²
                    time_slots = await self.extract_time_slots(date)
                    
                    # Ð”Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð² Ð¾Ð±Ñ‰Ð¸Ð¹ ÑÐ¿Ð¸ÑÐ¾Ðº
                    all_data.extend(time_slots)
                    
                    # Ð˜Ð¼Ð¸Ñ‚Ð°Ñ†Ð¸Ñ Ð¿Ð¾Ð²ÐµÐ´ÐµÐ½Ð¸Ñ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ: ÑÐ»ÑƒÑ‡Ð°Ð¹Ð½Ð°Ñ Ð·Ð°Ð´ÐµÑ€Ð¶ÐºÐ° Ð¼ÐµÐ¶Ð´Ñƒ Ð·Ð°Ð¿Ñ€Ð¾ÑÐ°Ð¼Ð¸
                    await asyncio.sleep(self.browser_manager.get_random_delay(1, 3))
            
            success = len(all_data) > 0
            if success:
                self.last_parsed_urls[url] = datetime.now()
                logger.info(f"ÐŸÐ°Ñ€ÑÐ¸Ð½Ð³ URL: {url} Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½ ÑƒÑÐ¿ÐµÑˆÐ½Ð¾, Ð¿Ð¾Ð»ÑƒÑ‡ÐµÐ½Ð¾ {len(all_data)} Ð·Ð°Ð¿Ð¸ÑÐµÐ¹")
            else:
                logger.warning(f"ÐŸÐ°Ñ€ÑÐ¸Ð½Ð³ URL: {url} Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½, Ð½Ð¾ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð½Ðµ Ð¸Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ñ‹")
                
            return success, all_data
        
        except Exception as e:
            logger.error(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ Ð¿Ð°Ñ€ÑÐ¸Ð½Ð³Ðµ Ð¿Ñ€ÑÐ¼Ð¾Ð¹ ÑÑÑ‹Ð»ÐºÐ¸ {url}: {str(e)}")
            return False, []
    
    def is_yclients_url(self, url: str) -> bool:
        """ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÑ‚, ÑÐ²Ð»ÑÐµÑ‚ÑÑ Ð»Ð¸ URL ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†ÐµÐ¹ YClients."""
        yclients_indicators = [
            'yclients.com',
            'record-type',
            'personal/',
            'select-time',
            'select-master'
        ]
        return any(indicator in url for indicator in yclients_indicators)

    async def parse_all_urls(self) -> Dict[str, List[Dict[str, Any]]]:
        """
        ÐŸÐ°Ñ€ÑÐ¸Ð½Ð³ Ð´Ð°Ð½Ð½Ñ‹Ñ… ÑÐ¾ Ð²ÑÐµÑ… URL.
        
        Returns:
            Dict[str, List[Dict[str, Any]]]: Ð¡Ð»Ð¾Ð²Ð°Ñ€ÑŒ Ñ Ð´Ð°Ð½Ð½Ñ‹Ð¼Ð¸ Ð´Ð»Ñ ÐºÐ°Ð¶Ð´Ð¾Ð³Ð¾ URL
        """
        logger.info("ÐÐ°Ñ‡Ð°Ð»Ð¾ Ð¿Ð°Ñ€ÑÐ¸Ð½Ð³Ð° Ð²ÑÐµÑ… URL")
        results = {}
        
        try:
            await self.initialize()
            
            for url in self.urls:
                retry_count = 0
                success = False
                data = []
                
                # Ð”ÐµÐ»Ð°ÐµÐ¼ Ð½ÐµÑÐºÐ¾Ð»ÑŒÐºÐ¾ Ð¿Ð¾Ð¿Ñ‹Ñ‚Ð¾Ðº Ð¿Ð°Ñ€ÑÐ¸Ð½Ð³Ð° Ñ Ñ€Ð°Ð·Ð½Ñ‹Ð¼Ð¸ Ð¿Ñ€Ð¾ÐºÑÐ¸
                while not success and retry_count < MAX_RETRIES:
                    success, data = await self.parse_url(url)
                    
                    if not success:
                        retry_count += 1
                        logger.warning(f"ÐŸÐ¾Ð¿Ñ‹Ñ‚ÐºÐ° {retry_count}/{MAX_RETRIES} Ð´Ð»Ñ {url} Ð½Ðµ ÑƒÐ´Ð°Ð»Ð°ÑÑŒ, ÑÐ¼ÐµÐ½Ð° Ð¿Ñ€Ð¾ÐºÑÐ¸")
                        
                        # Ð—Ð°ÐºÑ€Ñ‹Ð²Ð°ÐµÐ¼ Ñ‚ÐµÐºÑƒÑ‰Ð¸Ð¹ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚ Ð¸ Ð±Ñ€Ð°ÑƒÐ·ÐµÑ€
                        await self.close()
                        
                        # ÐœÐµÐ½ÑÐµÐ¼ Ð¿Ñ€Ð¾ÐºÑÐ¸ Ð¸ Ð¸Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÐµÐ¼ Ð½Ð¾Ð²Ñ‹Ð¹ Ð±Ñ€Ð°ÑƒÐ·ÐµÑ€
                        self.current_proxy = self.proxy_manager.get_next_proxy()
                        self.browser, self.context = await self.browser_manager.initialize_browser(
                            proxy=self.current_proxy
                        )
                    else:
                        # Ð•ÑÐ»Ð¸ ÑƒÑÐ¿ÐµÑˆÐ½Ð¾, ÑÐ¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ Ð´Ð°Ð½Ð½Ñ‹Ðµ
                        results[url] = data
                
                # Ð•ÑÐ»Ð¸ Ð²ÑÐµ Ð¿Ð¾Ð¿Ñ‹Ñ‚ÐºÐ¸ Ð½ÐµÑƒÐ´Ð°Ñ‡Ð½Ñ‹, Ð·Ð°Ð¿Ð¸ÑÑ‹Ð²Ð°ÐµÐ¼ Ð¿ÑƒÑÑ‚Ð¾Ð¹ ÑÐ¿Ð¸ÑÐ¾Ðº
                if not success:
                    logger.error(f"ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ñ‚ÑŒ URL {url} Ð¿Ð¾ÑÐ»Ðµ {MAX_RETRIES} Ð¿Ð¾Ð¿Ñ‹Ñ‚Ð¾Ðº")
                    results[url] = []
                
            logger.info(f"ÐŸÐ°Ñ€ÑÐ¸Ð½Ð³ Ð²ÑÐµÑ… URL Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½, Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ð½Ð¾ {len(results)} URL")
        
        except Exception as e:
            logger.error(f"ÐšÑ€Ð¸Ñ‚Ð¸Ñ‡ÐµÑÐºÐ°Ñ Ð¾ÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ Ð¿Ð°Ñ€ÑÐ¸Ð½Ð³Ðµ URL: {str(e)}")
        finally:
            await self.close()
        
        return results

    async def run_single_iteration(self) -> None:
        """Ð’Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ðµ Ð¾Ð´Ð½Ð¾Ð¹ Ð¸Ñ‚ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ð¿Ð°Ñ€ÑÐ¸Ð½Ð³Ð° Ð²ÑÐµÑ… URL."""
        logger.info("ÐÐ°Ñ‡Ð°Ð»Ð¾ Ð¸Ñ‚ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ð¿Ð°Ñ€ÑÐ¸Ð½Ð³Ð°")
        start_time = time.time()
        
        try:
            # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ Ð´Ð°Ð½Ð½Ñ‹Ðµ ÑÐ¾ Ð²ÑÐµÑ… URL
            results = await self.parse_all_urls()
            
            # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ Ð¿Ð¾Ð»ÑƒÑ‡ÐµÐ½Ð½Ñ‹Ðµ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð² Ð±Ð°Ð·Ñƒ Ð´Ð°Ð½Ð½Ñ‹Ñ…
            for url, data in results.items():
                if data:
                    logger.info(f"Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ðµ {len(data)} Ð·Ð°Ð¿Ð¸ÑÐµÐ¹ Ð´Ð»Ñ URL {url}")
                    await self.db_manager.save_booking_data(url, data)
                else:
                    logger.warning(f"ÐÐµÑ‚ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð´Ð»Ñ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ñ Ð´Ð»Ñ URL {url}")
            
        except Exception as e:
            logger.error(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ð¸ Ð¸Ñ‚ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ð¿Ð°Ñ€ÑÐ¸Ð½Ð³Ð°: {str(e)}")
        
        elapsed_time = time.time() - start_time
        logger.info(f"Ð˜Ñ‚ÐµÑ€Ð°Ñ†Ð¸Ñ Ð¿Ð°Ñ€ÑÐ¸Ð½Ð³Ð° Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð° Ð·Ð° {elapsed_time:.2f} ÑÐµÐºÑƒÐ½Ð´")

    async def run_continuous(self) -> None:
        """ÐÐµÐ¿Ñ€ÐµÑ€Ñ‹Ð²Ð½Ñ‹Ð¹ Ð¿Ð°Ñ€ÑÐ¸Ð½Ð³ Ñ Ð·Ð°Ð´Ð°Ð½Ð½Ñ‹Ð¼ Ð¸Ð½Ñ‚ÐµÑ€Ð²Ð°Ð»Ð¾Ð¼."""
        logger.info(f"Ð—Ð°Ð¿ÑƒÑÐº Ð½ÐµÐ¿Ñ€ÐµÑ€Ñ‹Ð²Ð½Ð¾Ð³Ð¾ Ð¿Ð°Ñ€ÑÐ¸Ð½Ð³Ð° Ñ Ð¸Ð½Ñ‚ÐµÑ€Ð²Ð°Ð»Ð¾Ð¼ {PARSE_INTERVAL} ÑÐµÐºÑƒÐ½Ð´")
        
        while True:
            try:
                await self.run_single_iteration()
                logger.info(f"ÐžÐ¶Ð¸Ð´Ð°Ð½Ð¸Ðµ {PARSE_INTERVAL} ÑÐµÐºÑƒÐ½Ð´ Ð´Ð¾ ÑÐ»ÐµÐ´ÑƒÑŽÑ‰ÐµÐ¹ Ð¸Ñ‚ÐµÑ€Ð°Ñ†Ð¸Ð¸")
                await asyncio.sleep(PARSE_INTERVAL)
            
            except KeyboardInterrupt:
                logger.info("ÐŸÐ¾Ð»ÑƒÑ‡ÐµÐ½ ÑÐ¸Ð³Ð½Ð°Ð» Ð¾ÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ¸, Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð¸Ðµ Ñ€Ð°Ð±Ð¾Ñ‚Ñ‹")
                break
            
            except Exception as e:
                logger.error(f"ÐÐµÐ¿Ñ€ÐµÐ´Ð²Ð¸Ð´ÐµÐ½Ð½Ð°Ñ Ð¾ÑˆÐ¸Ð±ÐºÐ° Ð² Ñ†Ð¸ÐºÐ»Ðµ Ð¿Ð°Ñ€ÑÐ¸Ð½Ð³Ð°: {str(e)}")
                # ÐÐµÐ±Ð¾Ð»ÑŒÑˆÐ°Ñ Ð¿Ð°ÑƒÐ·Ð° Ð¿ÐµÑ€ÐµÐ´ ÑÐ»ÐµÐ´ÑƒÑŽÑ‰ÐµÐ¹ Ð¿Ð¾Ð¿Ñ‹Ñ‚ÐºÐ¾Ð¹ Ð² ÑÐ»ÑƒÑ‡Ð°Ðµ Ð¾ÑˆÐ¸Ð±ÐºÐ¸
                await asyncio.sleep(10)


async def main():
    """ÐŸÑ€Ð¸Ð¼ÐµÑ€ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸Ñ Ð¿Ð°Ñ€ÑÐµÑ€Ð°."""
    from src.database.db_manager import DatabaseManager
    
    # ÐŸÑ€Ð¸Ð¼ÐµÑ€ URL Ð´Ð»Ñ Ð¿Ð°Ñ€ÑÐ¸Ð½Ð³Ð°
    urls = [
        "https://yclients.com/company/111111/booking",
        "https://yclients.com/company/222222/booking"
    ]
    
    # Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð¼ÐµÐ½ÐµÐ´Ð¶ÐµÑ€Ð° Ð±Ð°Ð·Ñ‹ Ð´Ð°Ð½Ð½Ñ‹Ñ…
    db_manager = DatabaseManager()
    await db_manager.initialize()
    
    # Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð¿Ð°Ñ€ÑÐµÑ€Ð°
    parser = YClientsParser(urls, db_manager)
    
    # Ð—Ð°Ð¿ÑƒÑÐº Ð¾Ð´Ð½Ð¾Ð¹ Ð¸Ñ‚ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ð¿Ð°Ñ€ÑÐ¸Ð½Ð³Ð°
    await parser.run_single_iteration()
    
    # Ð—Ð°ÐºÑ€Ñ‹Ñ‚Ð¸Ðµ ÑÐ¾ÐµÐ´Ð¸Ð½ÐµÐ½Ð¸Ñ Ñ Ð±Ð°Ð·Ð¾Ð¹ Ð´Ð°Ð½Ð½Ñ‹Ñ…
    await db_manager.close()


if __name__ == "__main__":
    # ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ° Ð»Ð¾Ð³Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
    )
    
    # Ð—Ð°Ð¿ÑƒÑÐº Ð¾ÑÐ½Ð¾Ð²Ð½Ð¾Ð¹ Ñ„ÑƒÐ½ÐºÑ†Ð¸Ð¸
    asyncio.run(main())
